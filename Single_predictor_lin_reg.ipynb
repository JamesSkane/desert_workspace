{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/desert/desert_workspace/final_data.csv')\n",
    "df=df.drop('Unnamed: 0', axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=df[df.columns.tolist()[2:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_dataframe = data.corr()\n",
    "\n",
    "# compute hierarchical cluster on both rows and columns for correlation matrix and plot heatmap \n",
    "def corr_heatmap(corr_dataframe):\n",
    "    import scipy.cluster.hierarchy as sch\n",
    "    \n",
    "    corr_matrix = np.array(corr_dataframe)\n",
    "    col_names = corr_dataframe.columns\n",
    "    \n",
    "    Y = sch.linkage(corr_matrix, 'single', 'correlation')\n",
    "    Z = sch.dendrogram(Y, color_threshold=0, no_plot=True)['leaves']\n",
    "    corr_matrix = corr_matrix[Z, :]\n",
    "    corr_matrix = corr_matrix[:, Z]\n",
    "    col_names = col_names[Z]\n",
    "    im = plt.imshow(corr_matrix, interpolation='nearest', aspect='auto', cmap='bwr')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(corr_matrix.shape[0]), col_names, rotation='vertical', fontsize=4)\n",
    "    plt.yticks(range(corr_matrix.shape[0]), col_names[::-1], fontsize=4)\n",
    "    \n",
    "# plot\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_high_corr(corr_dataframe, thresh = 0.9):\n",
    "    '''remove predictors with high pairwise correlation'''\n",
    "    abs_corr = np.abs(corr_dataframe).as_matrix() # absolute correlation matrix\n",
    "    col_names = list(corr_dataframe.columns)\n",
    "    \n",
    "    # set up diagonal to 0\n",
    "    np.fill_diagonal(abs_corr, 0)\n",
    "    \n",
    "    print \"Removed predictors (in order): \\n\"\n",
    "    while np.max(abs_corr) >= thresh:\n",
    "        i, j = np.unravel_index(abs_corr.argmax(), abs_corr.shape) # find maximum element\n",
    "        # print abs_corr[i, j]\n",
    "        rdx = which_to_remove(i, j, abs_corr)\n",
    "        # remove corresponding predictor\n",
    "        print col_names.pop(rdx)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 0)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 1)\n",
    "        \n",
    "    return col_names\n",
    "\n",
    "def which_to_remove(i, j, abs_corr):\n",
    "    '''compare two predictors and remove the one with higher abs correlation with other predictors'''\n",
    "    i_absmean = np.mean(abs_corr[i, np.where(abs_corr[i,:] == 0)])\n",
    "    j_absmean = np.mean(abs_corr[j, np.where(abs_corr[j,:] == 0)])\n",
    "    \n",
    "    return i if i_absmean > j_absmean else j\n",
    "\n",
    "# remained predictors\n",
    "col_remained = remove_high_corr(corr_dataframe)\n",
    "data=data[col_remained]\n",
    "corr_dataframe = data.corr()\n",
    "\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_remained = remove_high_corr(corr_dataframe)\n",
    "data = df[col_remained]\n",
    "corr_dataframe = data.corr()\n",
    "\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = data.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns.tolist()\n",
    "\n",
    "\n",
    "model = forward_selected(data, 'pop2010_in_des')\n",
    "\n",
    "print model.model.formula\n",
    "\n",
    "print model.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from bokeh.charts import Histogram\n",
    "from bokeh.charts import defaults, vplot, hplot, show, output_file\n",
    "from bokeh.io import output_notebook \n",
    "output_notebook()\n",
    "\n",
    "\n",
    "# input options\n",
    "hist = Histogram(df['pop2010_in_des'], title=\"df['pop2010_in_des']\")\n",
    "show(hist)\n",
    "#df['pop2010_in_des']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['percent_food_desert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_expected_value = df['percent_food_desert'].mean()\n",
    "Squared_errors = pd.Series(mean_expected_value - df['percent_food_desert'])**2\n",
    "SSE = np.sum(Squared_errors)\n",
    "print ('Sum of Squared Errors (SSE): %01.f' % SSE)\n",
    "density_plot = Squared_errors.plot('hist')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows how frequent certain errors are in respect of their values. Therefore, we can see that most errors are around zero (there is a high density around that value). Such a situation can be considered a good one, since in most cases the mean is a good approximation, but some errors are really very far from the zero and they can attain considerable values (don't forget that the errors are squared, anyway, so the effect is emphasized). When trying to  gure out such values, our approach will surely lead to a relevant error and we should find a way to minimize it using a more sophisticated approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the mean is not a good representative of certain values, but it is certainly a good baseline to start from. Certainly, an important problem with the mean is\n",
    "its being  xed, whereas the target variable is changeable. However, if we assume that the target variable changes because of the effect of some other variable we are measuring, then we can adjust the mean with respect to the variations in cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One improvement on our previous approach could be to build a mean conditional on certain values of another variable (or even more than one) actually related to our target, whose variation is somehow similar to the variation of the target one.  \n",
    "\n",
    "Intuitively, if we know the dynamics we want to predict with our model, we can try to look for variables that we know can impact the answer values.   \n",
    "\n",
    "In food deserts, we actually know that usually the larger a county population is,\n",
    "the more food desert census tracts it will posess; however, this rule is just part of the story and the number of food deserts possess by a county is affected by many other considerations. For the moment, we will keep it simple and just assume that the unemployment rate is a factor that negatively affects a counties well being, and consequently, results in more food deserts.  \n",
    "\n",
    "Now, we have a variable that we know should change with our target and we just need to measure it and extend our initial formula based on constant values with something else.\n",
    "In statistics, there is a measure that helps to measure how (in the sense of how much and in what direction) two variables relate to each other: \n",
    "#### correlation. ####\n",
    "In correlation, a few steps are to be considered. First, your variables have to be standardized (or your result won't be a correlation but a covariation, a measure of association that is affected by the scale of the variables you are working with)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistical Z score standardization, you subtract from each variable its mean and then you divide the result by the standard deviation. The resulting transformed variable will have a mean of 0 and a standard deviation of 1 (or unit variance, since variance is the squared standard deviation).\n",
    "The formula for standardizing a variable is as follows:\n",
    "\n",
    "$$x = x-mean(x) / std(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardizing, you compare the squared difference of each variable with its own mean. If the two differences agree in sign, their multiplication will become positive (evidence that they have the same directionality); however, if they differ, the multiplication will turn negative. By summing all the multiplications between the squared differences, and dividing them by the number of observations, you will  finally get the correlation which will be a number ranging from -1 to 1.\n",
    "The absolute value of the correlation will provide you with the intensity of the relation between the two variables compared, 1 being a sign of a perfect match and zero a sign of complete independence between them (they have no relation between them). The sign instead will hint at the proportionality; positive is direct (when one grows the other does the same), negative is indirect (when one grows, the other shrinks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Covariance can be expressed as follows:\n",
    "\n",
    "$$ cov(xi,y) = 1/n * sum(xi - mean(x1) * (y-mean(y)) $$\n",
    "\n",
    "Whereas, Pearson's correlation can be expressed as follows:\n",
    "\n",
    "$$ r = 1/n * {sum(xi - mean(x1) * (y-mean(y)) / std(xi * std(y)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covariance(variable_1, variable_2, bias=0):\n",
    "    observations = float(len(variable_1))\n",
    "    return np.sum((variable_1 - np.mean(variable_1)) * (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n",
    "\n",
    "def standardize(variable):\n",
    "    return (variable - np.mean(variable)) / np.std(variable)\n",
    "\n",
    "def correlation(var1,var2,bias=0):\n",
    "    return covariance(standardize(var1), standardize(var2),bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "print ('Our correlation estimation: %0.5f' % (correlation(df['unemployment_rate'], df['percent_food_desert'])))\n",
    "print ('Correlation from Scipy pearsonr estimation: %0.5f' % pearsonr(df['unemployment_rate'], df['percent_food_desert'])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our correlation estimation for the relation between the value of the target variable and the county unemployment rate in the area is 0.45024, which is positive and considerably strong, since the maximum positive score of a correlation is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_range = [df['unemployment_rate'].min(),df['unemployment_rate'].max()]\n",
    "y_range = [df['percent_food_desert'].min(),df['percent_food_desert'].max()]\n",
    "scatter_plot = df.plot(kind='scatter', x='unemployment_rate', y='percent_food_desert', xlim=x_range, ylim=y_range)\n",
    "meanY = scatter_plot.plot(x_range, [df['percent_food_desert'].mean(),df['percent_food_desert'].mean()], '--' , color='red', linewidth=1)\n",
    "meanX = scatter_plot.plot([df['unemployment_rate'].mean(),df['unemployment_rate'].mean()], y_range, '--', color='red', linewidth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['percent_food_desert']\n",
    "X = df['unemployment_rate']\n",
    "linear_regression = smf.ols(formula='percent_food_desert ~ unemployment_rate', data=df)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (fitted_model.params)\n",
    "betas = np.array(fitted_model.params)\n",
    "fitted_values = fitted_model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Dep. Variable: It just reminds you what the target variable was  \n",
    "• Model: Another reminder of the model that you have fitted, the OLS is\n",
    "ordinary least squares, another way to refer to linear regression   \n",
    "• Method: The parameters fitting method (in this case least squares, the classical computation method)   \n",
    "• No. Observations: The number of observations that have been used   \n",
    "• DF Residuals: The degrees of freedom of the residuals, which is the number\n",
    "of observations minus the number of parameters     \n",
    "• DF Model: The number of estimated parameters in the model (excluding the constant term from the count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second table gives a more interesting picture, focusing how good the  t of the linear regression model is and pointing out any possible problems with the model:  \n",
    "  \n",
    "• R-squared: This is the coefficient of determination, a measure of how well the regression does with respect to a simple mean.  \n",
    "• Adj. R-squared: This is the coefficient of determination adjusted based on the number of parameters in a model and the number of observations that helped build it.   \n",
    "• F-statistic: This is a measure telling you if, from a statistical point of view, all your coefficients, apart from the bias and taken together, are different from zero. In simple words, it tells you if your regression is really better than a simple average.   \n",
    "• Prob (F-statistic): This is the probability that you got that F-statistic just by lucky chance due to the observations that you have used (such a probability is actually called the p-value of F-statistic). If it is low enough you can be confident that your regression is really better than a simple mean. Usually in statistics and science a test probability has to be equal or lower than 0.05    \n",
    "(a conventional criterion of statistical significance) for having such a confidence.\n",
    "• AIC: This is the Akaike Information Criterion. AIC is a score that evaluates the model based on the number of observations and the complexity of\n",
    "the model itself. The lesser the AIC score, the better. It is very useful for comparing different models and for statistical variable selection.   \n",
    "• BIC: This is the Bayesian Information Criterion. It works as AIC, but it presents a higher penalty for models with more parameters.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these statistics make sense when we are dealing with more than one predictor variable, so they will be discussed in the next notebook. Thus, for the moment, as we are working with a simple linear regression, the two measures that are worth examining closely are F-statistic and R-squared.\n",
    "_______\n",
    "* F-statistic is actually a test that doesn't tell you too much if you have enough observations and you can count on a minimally correlated predictor variable. Usually it shouldn't be much of a concern in a data science project.  \n",
    "\n",
    "* R-squared is instead much more interesting because it tells you how much better your regression model is in comparison to a single mean. It does so by providing you with a percentage of the unexplained variance of a mean as a predictor that actually your model was able to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute the measure yourself, you just have to calculate the sum of squared errors of the mean of the target variable. That's your baseline of unexplained variance (the variability in the percentage of population living in a food desert that in our example we want to explain by a model). If from that baseline you subtract the sum of squared errors of your regression model, you will get the residual sum of squared errors, which can be compared using a division with your baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_sum_squared_errors = np.sum((df['percent_food_desert']-df['percent_food_desert'].mean())**2)\n",
    "regr_sum_squared_errors = np.sum((df['percent_food_desert']-fitted_values)**2)\n",
    "(mean_sum_squared_errors-regr_sum_squared_errors) / mean_sum_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_sum_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr_sum_squared_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second output table informs us about the coef cients and provides us with a series of tests. These tests can make us con dent that we have not been fooled by a few extreme observations in the foundations of our analysis or by some other problem:\n",
    "• coef: The estimated coefficient  \n",
    "• std err: The standard error of the estimate of the coefficient; the larger it is,\n",
    "the more uncertain the estimation of the coefficient  \n",
    "• t: The t-statistic value, a measure indicating whether the coefficient true value is different from zero  \n",
    "• P > |t|: The p-value indicating the probability that the coefficient is different from zero just by chance  \n",
    "• [95.0% Conf. Interval]: The lower and upper values of the coefficient, considering 95% of all the chances of having different observations and so different estimated coefficients  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = -0.1220 * x_{unemploymentrate} * 0.0260$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Folmula applied to Alameda county\n",
    "-0.1220 * 8.208532 * 0.0260\n",
    "# Close to 0.037134?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a linear regression can always work within the range of values it learned from (this is called interpolation) but can provide correct values for its learning boundaries\n",
    "(a different predictive activity called extrapolation) only in certain conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors instead are very important because they signal a weak or unclear relationship between the predictor and the answer. You can notice this by dividing the standard error by its beta. If the ratio is 0.5 or even larger, then it's a clear sign that the model has little con dence that it provided you with the right coef cient estimates. Having more cases is always the solution because it can reduce the standard errors of the coef cients and improve our estimates; however, there are also other methods to reduce errors, such as removing the redundant variance present among the features by a principal component analysis or selecting a parsimonious set of predictors by greedy selections. All these topics will be discussed when we work with multiple predictors; at this point in the book, we will illustrate the remedies to such a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regr_sum_squared_errors/ betas\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last table deals with an analysis of the residuals of the regression. The residuals are the difference between the target values and the predicted  tted values:\n",
    "  \n",
    "• Skewness: This is a measure of the symmetry of the residuals around the mean. For symmetric distributed residuals, the value should be around zero. A positive value indicates a long tail to the right; a negative value a long tail to the left.  \n",
    "• Kurtosis: This is a measure of the shape of the distribution of the residuals. A bell-shaped distribution has a zero measure. A negative value points to a too flat distribution; a positive one has too great a peak.  \n",
    "• Omnibus D'Angostino's test: This is a combined statistical test for skewness and kurtosis.  \n",
    "• Prob(Omnibus): This is the Omnibus statistic turned into a probability.  \n",
    "• Jarque-Bera: This is another test of skewness and kurtosis.  \n",
    "• Prob (JB): This is the JB statistic turned into a probability.  \n",
    "• Durbin-Watson: This is a test for the presence of correlation among the residuals (relevant during analysis of time-based data).  \n",
    "• Cond. No: This is a test for multicollinearity (we will deal with the concept of multicollinearity when working with many predictors).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = df['percent_food_desert']-fitted_values\n",
    "normalized_residuals = standardize(residuals)\n",
    "residual_scatter_plot = plt.plot(df['unemployment_rate'], normalized_residuals,'bp')\n",
    "mean_residual = plt.plot([int(x_range[0]),round(x_range[1],0)], [0,0],'-', color='red', linewidth=2)\n",
    "upper_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [3,3], '--', color='red', linewidth=1)\n",
    "lower_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [-3,-3],'--', color='red', linewidth=1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='unemployment_rate', y='percent_food_desert')\n",
    "plt.plot(pd.DataFrame(df['unemployment_rate']),fitted_values,c='red',linewidth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_partregress_grid(fitted_model, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_fit(fitted_model, \"unemployment_rate\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = sm.graphics.plot_leverage_resid2(fitted_model, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_ccpr_grid(fitted_model, fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the Cost Function\n",
    "---\n",
    "At the core of linear regression, there is the search for a line's equation that it is able to minimize the sum of the squared errors of the difference between the line's y values and the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_cost(v,e):\n",
    "    return np.sum((v-e)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha=0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "\n",
    "def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)    \n",
    "    return w, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations = len(df)\n",
    "X  = df['unemployment_rate'].values.reshape((observations,1))\n",
    "# X should be always a matrix, never a vector\n",
    "X = np.column_stack((X,np.ones(observations))) # We add the bias\n",
    "y  = df['percent_food_desert'].values # y can be a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.048\n",
    "w, path = optimize(X,y,alpha, eta = 10**-12, iterations = 100)\n",
    "print (\"These are our final coefficients: %s\" % w)\n",
    "print (\"Obtained walking on this path of squared loss %s\" % path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df['unemployment_rate'].values\n",
    "X  = df['unemployment_rate'].values.reshape((observations,1))\n",
    "# X should be always a matrix, never a vector\n",
    "X = np.column_stack((X,np.ones(observations))) # We add the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "observations = len(df)\n",
    "#variables = 'pop2010_in_des\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.std_\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))\n",
    "y  = df['pop2010_in_des'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.02\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 200)\n",
    "print (\"These are our final standardized coefficients: \" + ', \\\n",
    "'.join(map(lambda x: \"%0.4f\" % x, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cors = pd.DataFrame(df.corr()[\"percent_food_desert\"]).dropna()\n",
    "cors.sort_values(by='percent_food_desert').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_cols=['unemployment_rate', 'unemployment_rate', 'cnty_obesity_pct_adj','cnty_inactive_pct','cnty_obesity_pct', 'PCT_18_64','p_hs_edatt','NUMGQTRS'] \n",
    "#x_cols=['PCT_18_64','p_hs_edatt','NUMGQTRS','cnty_inactive_pct'] \n",
    "\n",
    "df[x_cols]\n",
    "y = df['percent_food_desert']\n",
    "X = df[x_cols]\n",
    "X = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y,X)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(X[['unemployment_rate', 'cnty_obesity_pct_adj','cnty_inactive_pct','cnty_obesity_pct', 'PCT_18_64','p_hs_edatt']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.info(verbose=True)\n",
    "data=df[['percent_food_desert','unemployment_rate','cnty_inactive_pct','cnty_obesity_pct', 'PCT_18_64','p_hs_edatt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_cols=['unemployment_rate','cnty_inactive_pct','cnty_obesity_pct', 'PCT_18_64','p_hs_edatt']\n",
    "df[x_cols]\n",
    "y = df['percent_food_desert']\n",
    "X = df[x_cols]\n",
    "X = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y,X)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_cols=[ 'percent_food_desert','unemployment_rate', 'unemployment_rate', 'cnty_obesity_pct_adj','cnty_inactive_pct','cnty_obesity_pct', 'PCT_18_64','p_hs_edatt','NUMGQTRS'] \n",
    "#x_cols=['PCT_18_64','p_hs_edatt','NUMGQTRS','cnty_inactive_pct'] \n",
    "\n",
    "data = df[x_cols]\n",
    "\n",
    "model = forward_selected(data, 'percent_food_desert')\n",
    "\n",
    "print model.model.formula\n",
    "\n",
    "print model.rsquared_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()\n",
    "cols=['opiods_rx_1000', 'pop2010_in_des',\n",
    "'num_tracts',\n",
    "'n_food_des',\n",
    "'n_urban',\n",
    "'n_rural',\n",
    "'Rural',\n",
    "'Urban',\n",
    "'cnty_obesity_pct',\n",
    "'cnty_dm_pct',\n",
    "'cnty_inactive_pct',\n",
    "'POP2010',\n",
    "'OHU2010',\n",
    "'NUMGQTRS',\n",
    "'HUNVFlag',\n",
    "'Adolescent_births',\n",
    "'ABR',\n",
    "'p_hs_edatt',\n",
    "'PC_PHYS_R',\n",
    "'DENTIST_R',\n",
    "'PSYCH_R',\n",
    "'PCT_HSPNC',\n",
    "'PCT_WHITE',\n",
    "'PCT_BLACK',\n",
    "'PCT_ASIAN',\n",
    "'PCT_AMIND_ESK',\n",
    "'PCT_ISLANDER',\n",
    "'PCT_MULTI',\n",
    "'PCT_OTHER',\n",
    "'PCT_65OVER',\n",
    "'PCT_18_64',\n",
    "'PCT_UNDR18',\n",
    "'PCT_UNDER5',\n",
    "'unemployment_rate',\n",
    "'n_hospitals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data =df[cols].dropna()\n",
    "data\n",
    "# model = forward_selected(data, 'pop2010_in_des')\n",
    "\n",
    "# print model.model.formula\n",
    "\n",
    "# print model.rsquared_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "#Load boston housing dataset as an example\n",
    "X =data.drop(\"pop2010_in_des\",axis=1)\n",
    "X=X.values\n",
    "Y = data[\"pop2010_in_des\"].values\n",
    "names = data.columns\n",
    " \n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "    score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\", cv=ShuffleSplit(len(X), 3, .3))\n",
    "    scores.append((round(np.mean(score), 3), names[i]))\n",
    "print sorted(scores, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats=pd.DataFrame(sorted(scores, reverse=True),columns=['importance','feature'])\n",
    "feats.plot(kind='bar')\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_importances(X_train, importances, f_rank):\n",
    "    '''\n",
    "    :param X_train: Dataframe containing predictor variables of training set\n",
    "    :param importances: List of feature importance, obtained from try_forest\n",
    "    :param f_rank: Feature ranking, obtained from sort_features()\n",
    "    :return: None, plot saved as 'Forest_feature_importances.png'\n",
    "    '''\n",
    "    plt.title('Feature Importances according to Random Forest')\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], color='lightblue', align='center')\n",
    "    plt.xticks(range(X_train.shape[1]), f_rank, rotation=45)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=feats['feature'][:5],y=feats['importance'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=feats['feature'][-5:],y=feats['importance'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "class LinReg:\n",
    "    kind='Linear Regression'\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.var=len(df.columns)\n",
    "        self.obs=len(df.index)\n",
    "        self.df=df\n",
    "\n",
    "    def model(self,x,y):\n",
    "        X=x[0]\n",
    "        for i in range(len(x)-1):\n",
    "            X=X+'+'+x[i+1]\n",
    "        mod=y+'~'+X\n",
    "        return mod\n",
    "    \n",
    "    def linreg(self,x, y):\n",
    "        result=sm.ols(formula=self.model(x,y), data=self.df).fit()\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=LinReg(df)\n",
    "x=['n_rural','n_urban']\n",
    "\n",
    "print d.var\n",
    "print d.obs\n",
    "print d.model(x,'pop2010_in_des')\n",
    "fit=d.linreg(x,'pop2010_in_des')\n",
    "\n",
    "print fit.summary()\n",
    "print fit.params\n",
    "print fit.tvalues\n",
    "print fit.pvalues\n",
    "print fit.rsquared\n",
    "print fit.f_test(np.identity(len(x)+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import arange,array,ones,asarray,vstack,linalg\n",
    "from scipy import stats\n",
    "\n",
    "def liner_regress(x, y, type=1):\n",
    "    if (type==1) : \n",
    "        A = vstack([x, ones(len(x))]).T\n",
    "        slope, intercept = linalg.lstsq(A, y)[0]\n",
    "    else:\n",
    "        slope, intercept, r, p, std_err = stats.linregress(x,y)\n",
    "        #print 'r value', r_value\n",
    "        #print  'p_value', p_value\n",
    "        #print 'standard deviation', std_err\n",
    "\n",
    "    print 'y = '+str(slope)+' * x + '+str(intercept)\n",
    "   \n",
    "    liney = slope*x + intercept\n",
    "    return liney, slope, intercept \n",
    "\n",
    "\n",
    "def _chunks(lst, size):\n",
    "    for i in xrange(0, len(lst), size):\n",
    "        yield lst[i:i+size]\n",
    "\n",
    "def _liner_regression(x, y, chunks_size, reduce_fn):\n",
    "    ys = [reduce_fn(yy) for yy in _chunks(y, chunks_size)]\n",
    "    xs = [max(xx) for xx in _chunks(x, chunks_size)]\n",
    "    liney, slope, intercept = liner_regression(xs, ys)\n",
    "    liney = slope*x + intercept\n",
    "    return liney, slope, intercept\n",
    "\n",
    "def liner_regression_max(x,y, chunks_size):\n",
    "    return _liner_regression(x, y, chunks_size, max)\n",
    "\n",
    "def liner_regression_min(x,y, chunks_size):\n",
    "    return _liner_regression(x, y, chunks_size, min)\n",
    "\n",
    "liner_regress(df['n_rural'], df['pop2010_in_des'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "# For 3d plots. This import is necessary to have 3D plotting below\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# For statistics. Requires statsmodels 5.0 or more\n",
    "from statsmodels.formula.api import ols\n",
    "# Analysis of Variance (ANOVA) on linear models\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "##############################################################################\n",
    "# Generate and show the data\n",
    "x = np.linspace(-5, 5, 21)\n",
    "# We generate a 2D grid\n",
    "X, Y = np.meshgrid(x, x)\n",
    "\n",
    "# To get reproducable values, provide a seed value\n",
    "np.random.seed(1)\n",
    "\n",
    "# Z is the elevation of this 2D grid\n",
    "Z = -5 + 3*X - 0.5*Y + 8 * np.random.normal(size=X.shape)\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.coolwarm,\n",
    "                       rstride=1, cstride=1)\n",
    "ax.view_init(20, -120)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "##############################################################################\n",
    "# Multilinear regression model, calculating fit, P-values, confidence\n",
    "# intervals etc.\n",
    "\n",
    "# Convert the data into a Pandas DataFrame to use the formulas framework\n",
    "# in statsmodels\n",
    "\n",
    "# First we need to flatten the data: it's 2D layout is not relevent.\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "Z = Z.flatten()\n",
    "\n",
    "data = pandas.DataFrame({'x': X, 'y': Y, 'z': Z})\n",
    "\n",
    "# Fit the model\n",
    "model = ols(\"z ~ x + y\", data).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n",
    "\n",
    "print(\"\\nRetrieving manually the parameter estimates:\")\n",
    "print(model._results.params)\n",
    "# should be array([-4.99754526,  3.00250049, -0.50514907])\n",
    "\n",
    "# Peform analysis of variance on fitted linear model\n",
    "anova_results = anova_lm(model)\n",
    "\n",
    "print('\\nANOVA results')\n",
    "print(anova_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "class DataSet :\n",
    "    def __init__(self, X, Y) :\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def fn(self, function, *args) :\n",
    "        return function(self.X, self.Y, *args)\n",
    "\n",
    "def Sss(X, Y):\n",
    "    return numpy.float64(sum([X[i] * Y[i] - mean(X) * mean(Y) for i in range(0, len(X))]))\n",
    "\n",
    "def Sxx(X, Y) :\n",
    "    return Sss(X,X)\n",
    "\n",
    "def Syy(X, Y) :\n",
    "    return Sss(Y, Y)\n",
    "\n",
    "def Sxy(X, Y) :\n",
    "    return Sss(X, Y)\n",
    "\n",
    "def mean(X) :\n",
    "    return (numpy.float64(1.0) / numpy.float64(len(X))) * numpy.float64(sum(X))\n",
    "\n",
    "def slope(X, Y) :\n",
    "    return Sss(X, Y) / Sss(X, X)\n",
    "\n",
    "def intercept(X, Y) :\n",
    "    return mean(Y) - slope(X,Y) * mean(X)\n",
    "\n",
    "def expected(X, Y, x) :\n",
    "    return intercept(X, Y) + slope(X, Y) * x \n",
    "\n",
    "def residuals(X, Y) :\n",
    "    return [Y[i] - expected(X, Y, X[i]) for i in range(0, len(Y))]\n",
    "\n",
    "def SST(X, Y) :\n",
    "    return Sss(Y, Y)\n",
    "\n",
    "def SSR(X, Y) :\n",
    "    return (slope(X, Y) ** 2.0) * Sss(X, X)\n",
    "\n",
    "def SSE(X, Y) :\n",
    "    return SST(X, Y) - SSR(X, Y)\n",
    "\n",
    "def rsquared(X, Y) :\n",
    "    return 1 - SSE(X, Y) / SST(X, Y)\n",
    "\n",
    "def r(X, Y) :\n",
    "    rVal = math.sqrt(rsquared(X, Y))\n",
    "    if slope(X, Y) < 0.0 :\n",
    "        rVal *= -1.0\n",
    "\n",
    "    return rVal\n",
    "\n",
    "def variance(X, Y) :\n",
    "    return SSE(X, Y) / numpy.float64(len(X) - 2)\n",
    "\n",
    "def interceptError(X, Y) :\n",
    "    dev = numpy.float64(math.sqrt(variance(X, Y)))\n",
    "    a   = sum(X[i] * X[i] for i in range(0, len(X)))\n",
    "    b   = len(X) * Sss(X,X)\n",
    "\n",
    "    return dev * numpy.float64(math.sqrt(a / b))\n",
    "\n",
    "def slopeError(X, Y) :\n",
    "    dev = numpy.float64(math.sqrt(variance(X, Y)))\n",
    "    return dev / numpy.float64(math.sqrt(Sss(X, X)))\n",
    "\n",
    "def slopeCI(X, Y, alpha) :\n",
    "    s = slope(X, Y)\n",
    "    diff = abs(stats.t.ppf(alpha / 2.0, len(X) - 2)) * slopeError(X, Y)\n",
    "    return (s - diff, s + diff)\n",
    "\n",
    "def interceptCI(X, Y, alpha) :\n",
    "    i = intercept(X, Y)\n",
    "    diff = abs(stats.t.ppf(alpha / 2.0, len(X) - 2)) * interceptError(X, Y)\n",
    "    return (i - diff, i + diff)\n",
    "\n",
    "def slopeHypothesis(X, Y, null, alpha) :\n",
    "    diff = abs((slope(X, Y) - numpy.float64(null))) / slopeError(X, Y)\n",
    "    test = abs(stats.t.ppf(alpha / 2.0, len(X) - 2))\n",
    "\n",
    "    if diff > test :\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def MSR(X, Y) :\n",
    "    return SSR(X, Y)\n",
    "\n",
    "def MSE(X, Y) :\n",
    "    return SSE(X, Y) / numpy.float64(len(X) - 2)\n",
    "\n",
    "def ANOVA(X, Y) :\n",
    "    print \"(Source, SS, df, MS, F)\"\n",
    "    print \"Regression\", SSR(X, Y), \" \", 1, \" \", MSR(X, Y), \" \", MSR(X,Y) / MSE(X,Y)\n",
    "    print \"Error     \", SSE(X, Y), \" \", len(X) - 2, \" \", MSE(X,Y)\n",
    "    print \"Total     \", SST(X, Y), \" \", len(X) - 1\n",
    "\n",
    "def predictionInterval(X, Y, x, alpha) :\n",
    "    expect = expected(X, Y, numpy.float64(x))\n",
    "    tstat  = abs(stats.t.ppf(alpha / 2.0, len(X) - 2))\n",
    "    dev    = numpy.float64(math.sqrt(variance(X, Y)))\n",
    "    a      = 1.0 / numpy.float64(len(X))\n",
    "    b      = ((numpy.float64(x) - mean(X)) ** 2.0) / Sss(X,X)\n",
    "    diff   = tstat * dev * math.sqrt(1 + a + b)\n",
    "\n",
    "    return (expect - diff, expect + diff)\n",
    "\n",
    "def standardResiduals(X, Y) :\n",
    "    dev = math.sqrt(variance(X,Y))\n",
    "    return [residuals(X, Y)[i] / (dev * math.sqrt(1 - leverage(X, Y, i))) for i in range(0, len(X))]\n",
    "\n",
    "def outliers(X, Y) :\n",
    "    return [i for i in range(0, len(X)) if standardResiduals(X,Y)[i] > 2.0]\n",
    "\n",
    "def leverage(X, Y, i) :\n",
    "    return (1.0 / numpy.float64(len(X))) + ((X[i] - mean(X)) ** 2.0) / Sss(X, X)\n",
    "\n",
    "def influentials(X, Y) :\n",
    "    threshold = 2.0 * (2.0) / float(len(X))\n",
    "    return [i for i in range(0, len(X)) if leverage(X,Y,i) > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"\n",
    "    multivariate linear regression using gradient descent\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 50, verbose = True, l2 = 0, \n",
    "                tolerance = 0, intercept = True):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning rate constant\n",
    "        :param iterations: how many epochs\n",
    "        :param tolerance: the error value in which to stop training\n",
    "        :param intercept: whether to fit an intercept\n",
    "        :param verbose: whether to spit out error rates while training\n",
    "        :param l2: L2 regularization term\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.intercept = intercept\n",
    "        self.verbose = verbose\n",
    "        self.l2 = l2\n",
    "        self.theta = None\n",
    "        self.mean = []\n",
    "        self.std = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Gradient descent, loops over theta and updates to\n",
    "        take steps in direction of steepest decrease of J.\n",
    "        :return: value of theta that minimizes J(theta) and J_history\n",
    "        \"\"\"\n",
    "        if self.intercept:\n",
    "            intercept = np.ones((np.shape(X)[0],1))\n",
    "            X = np.concatenate((intercept, X), 1)\n",
    "            \n",
    "        num_examples, num_features = np.shape(X)\n",
    "\n",
    "        # initialize theta to 1\n",
    "        self.theta = np.ones(num_features)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # make prediction\n",
    "            predicted = np.dot(X, self.theta.T)\n",
    "            # update theta with gradient descent\n",
    "            self.theta = (self.theta * (1 - (self.learning_rate * self.l2))) - self.learning_rate / num_examples * np.dot((predicted - y).T, X)\n",
    "            # sum of squares cost\n",
    "            error = predicted - y\n",
    "            cost = np.sum(error**2) / (2 * num_examples)\n",
    "            \n",
    "            if i % 10 == 0 and self.verbose == True:\n",
    "                print 'iteration:', i\n",
    "                print 'theta:', self.theta\n",
    "                print 'cost:', cost\n",
    "                \n",
    "            if cost < self.tolerance:\n",
    "                return self.theta\n",
    "                break\n",
    "\n",
    "        return self.theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make linear prediction based on cost and gradient descent\n",
    "        :param X: new data to make predictions on\n",
    "        :return: return prediction\n",
    "        \"\"\"\n",
    "        if self.intercept:\n",
    "            intercept = np.ones((np.shape(X)[0],1))\n",
    "            X = np.concatenate((intercept, X), 1)\n",
    "        \n",
    "        num_examples, num_features = np.shape(X)\n",
    "        prediction = []\n",
    "        for sample in range(num_examples):\n",
    "            yhat = 0\n",
    "            for value in range(num_features):\n",
    "                yhat += X[sample, value] * self.theta[value]\n",
    "            prediction.append(yhat)\n",
    "                \n",
    "        return prediction\n",
    "\n",
    "def demo():\n",
    "    # initialize linear regression parameters\n",
    "    iterations = 2000\n",
    "    learning_rate = 0.1\n",
    "    l2 = 0.0001\n",
    "\n",
    "    linearReg = LinReg(learning_rate = learning_rate, iterations = iterations, verbose = 1, l2 = l2)\n",
    "\n",
    "    #data = np.genfromtxt('Data/blood_pressure.csv', delimiter = ',', skip_header = 1)\n",
    "    #X = data[:, 1:]\n",
    "    #y = data[:, 0]\n",
    "    y = data[\"pop2010_in_des\"].values\n",
    "\n",
    "    X =data.drop(\"pop2010_in_des\",axis=1)\n",
    "    \n",
    "    # scale data\n",
    "    max = np.amax(X)\n",
    "    X /= max\n",
    "    #print X\n",
    "    #print y\n",
    "\n",
    "    # fit the linear reg\n",
    "    linearReg.fit(X = X, y = y)\n",
    "\n",
    "    # load testing dataset\n",
    "    test = np.genfromtxt('Data/blood_pressure.csv', delimiter = ',', skip_header = 1)\n",
    "    X_test = test[:, 1:]\n",
    "    y_test = test[:, 0]\n",
    "    \n",
    "    max = np.amax(X_test)\n",
    "    X_test /= max\n",
    "    #print X_test\n",
    "\n",
    "    predictions = np.array(linearReg.predict(X_test))\n",
    "\n",
    "    #print 'correct: ', y_test\n",
    "    #print 'prediction: ', predictions\n",
    "    return y_test,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst=demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
