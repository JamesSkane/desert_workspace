{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import linear_model\n",
    "from regressors import stats\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn import datasets\n",
    "# boston = datasets.load_boston()\n",
    "# which_betas = np.ones(13, dtype=bool)\n",
    "# which_betas[3] = False  # Eliminate dummy variable\n",
    "# X = boston.data[:, which_betas]\n",
    "# y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ur=pd.read_csv('/Users/desert/desert_workspace/desert_data/urban_rural_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the dataset we made in our previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>POP2010</th>\n",
       "      <th>food_deserts</th>\n",
       "      <th>n_tracts</th>\n",
       "      <th>urban_des</th>\n",
       "      <th>rural_des</th>\n",
       "      <th>n_urban</th>\n",
       "      <th>n_rural</th>\n",
       "      <th>des_percent</th>\n",
       "      <th>perc_lali</th>\n",
       "      <th>...</th>\n",
       "      <th>food_dx</th>\n",
       "      <th>opiods_rx_1000</th>\n",
       "      <th>opiods_greater_than_stateavg</th>\n",
       "      <th>MILK_PRICE10</th>\n",
       "      <th>SODA_PRICE10</th>\n",
       "      <th>MILK_SODA_PRICE10</th>\n",
       "      <th>PCH_FFR_07_12</th>\n",
       "      <th>FFR07</th>\n",
       "      <th>FFR12</th>\n",
       "      <th>pct_nonwhite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alameda</td>\n",
       "      <td>1510271</td>\n",
       "      <td>15</td>\n",
       "      <td>360</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>354</td>\n",
       "      <td>6</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>...</td>\n",
       "      <td>29.597337</td>\n",
       "      <td>557.20</td>\n",
       "      <td>0</td>\n",
       "      <td>1.122215</td>\n",
       "      <td>1.164422</td>\n",
       "      <td>0.891575</td>\n",
       "      <td>8.902878</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>1211</td>\n",
       "      <td>0.483413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alpine</td>\n",
       "      <td>1175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>578.68</td>\n",
       "      <td>0</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>0.877073</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amador</td>\n",
       "      <td>38091</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149514</td>\n",
       "      <td>...</td>\n",
       "      <td>13.126460</td>\n",
       "      <td>1244.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>0.877073</td>\n",
       "      <td>-5.555556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.055533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Butte</td>\n",
       "      <td>220000</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.145583</td>\n",
       "      <td>...</td>\n",
       "      <td>27.727273</td>\n",
       "      <td>1389.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996443</td>\n",
       "      <td>1.130546</td>\n",
       "      <td>0.815374</td>\n",
       "      <td>2.684564</td>\n",
       "      <td>149.0</td>\n",
       "      <td>153</td>\n",
       "      <td>0.096562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calaveras</td>\n",
       "      <td>45578</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203345</td>\n",
       "      <td>...</td>\n",
       "      <td>10.970205</td>\n",
       "      <td>1068.98</td>\n",
       "      <td>1</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>0.877073</td>\n",
       "      <td>-7.407407</td>\n",
       "      <td>27.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      County  POP2010  food_deserts  n_tracts  urban_des  rural_des  n_urban  \\\n",
       "0    Alameda  1510271            15       360       15.0        0.0      354   \n",
       "1     Alpine     1175             0         1        0.0        0.0        0   \n",
       "2     Amador    38091             0         9        0.0        0.0        0   \n",
       "3      Butte   220000            14        51       13.0        1.0       38   \n",
       "4  Calaveras    45578             0        10        0.0        0.0        2   \n",
       "\n",
       "   n_rural  des_percent  perc_lali      ...         food_dx  opiods_rx_1000  \\\n",
       "0        6     0.041667   0.018032      ...       29.597337          557.20   \n",
       "1        1     0.000000   0.266288      ...        0.000000          578.68   \n",
       "2        9     0.000000   0.149514      ...       13.126460         1244.76   \n",
       "3       13     0.274510   0.145583      ...       27.727273         1389.20   \n",
       "4        8     0.000000   0.203345      ...       10.970205         1068.98   \n",
       "\n",
       "   opiods_greater_than_stateavg  MILK_PRICE10  SODA_PRICE10  \\\n",
       "0                             0      1.122215      1.164422   \n",
       "1                             0      1.032918      1.089488   \n",
       "2                             1      1.032918      1.089488   \n",
       "3                             1      0.996443      1.130546   \n",
       "4                             1      1.032918      1.089488   \n",
       "\n",
       "   MILK_SODA_PRICE10  PCH_FFR_07_12   FFR07  FFR12  pct_nonwhite  \n",
       "0           0.891575       8.902878  1112.0   1211      0.483413  \n",
       "1           0.877073     -50.000000     2.0      1      0.310400  \n",
       "2           0.877073      -5.555556    18.0     17      0.055533  \n",
       "3           0.815374       2.684564   149.0    153      0.096562  \n",
       "4           0.877073      -7.407407    27.0     25      0.060800  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/desert/desert_workspace/desert_data/fd_data.csv')\n",
    "df = df.drop(['Unnamed: 0','State'], axis=1)\n",
    "\n",
    "\n",
    "infx = ['Chlamydia', 'Tuberculosis', 'Gonorrhea', 'HIV', 'senior_flu_deaths', 'Measles', 'Mumps', 'Pertussis', 'Rubella', 'varicella_hospitalizations', 'Salmonellosis', 'ecoli_hem', 'ecoli_nonhem', 'syphilis', 'botulism_dtfood']\n",
    "qi = ['mort_30_ami', 'mort_30_cabg', 'mort_30_copd', 'mort_30_hf', 'mort_30_pn', 'mort_30_stk', 'readm_30_ami', 'readm_30_cabg', 'readm_30_copd', 'readm_30_hf', 'readm_30_hip_knee', 'readm_30_hosp_wide', 'readm_30_pn', 'readm_30_stk']\n",
    "health_and_access = ['PC_PHYS_R', 'DENTIST_R', 'PSYCH_R','n_hospitals','cnty_obesity_pct_adj', 'cnty_dm_pct_adj', 'cnty_inactive_pct_adj', 'Adolescent_births', 'ABR', 'opiods_rx_1000', 'opiods_greater_than_stateavg','std', 'vaccine_dx', 'food_dx']\n",
    "race = ['PCT_HSPNC', 'PCT_BLACK', 'PCT_ASIAN', 'PCT_AMIND_ESK', 'PCT_ISLANDER', 'PCT_MULTI', 'PCT_OTHER']\n",
    "age = [ 'PCT_65OVER', 'PCT_18_64', 'PCT_UNDR18', 'PCT_UNDER5']\n",
    "social = ['total_housing_units', 'pop_in_group_housing', 'pop_effected', 'p_hs_edatt','MILK_PRICE10', 'SODA_PRICE10', 'MILK_SODA_PRICE10', 'PCH_FFR_07_12', 'FFR07', 'FFR12']\n",
    "base = ['County','des_percent', 'perc_lali']\n",
    "\n",
    "df['pct_nonwhite'] =  1 - df['PCT_WHITE'] \n",
    "#df = df.drop(race+infx, axis=1)\n",
    "cols = df.columns.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des_percent</th>\n",
       "      <th>perc_lali</th>\n",
       "      <th>food_deserts</th>\n",
       "      <th>urban_des</th>\n",
       "      <th>rural_des</th>\n",
       "      <th>n_urban</th>\n",
       "      <th>n_rural</th>\n",
       "      <th>LowIncomeTracts</th>\n",
       "      <th>low_vehicle_tracts</th>\n",
       "      <th>total_housing_units</th>\n",
       "      <th>...</th>\n",
       "      <th>readm_30_pn</th>\n",
       "      <th>readm_30_stk</th>\n",
       "      <th>std</th>\n",
       "      <th>vaccine_dx</th>\n",
       "      <th>food_dx</th>\n",
       "      <th>opiods_rx_1000</th>\n",
       "      <th>MILK_PRICE10</th>\n",
       "      <th>SODA_PRICE10</th>\n",
       "      <th>FFR07</th>\n",
       "      <th>pct_nonwhite</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>County</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alameda</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>15</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.147222</td>\n",
       "      <td>0.360954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178778</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>619.888748</td>\n",
       "      <td>41.648155</td>\n",
       "      <td>29.597337</td>\n",
       "      <td>557.20</td>\n",
       "      <td>1.122215</td>\n",
       "      <td>1.164422</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.483413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpine</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266288</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>578.68</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amador</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149514</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.382479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>199.522197</td>\n",
       "      <td>15.751752</td>\n",
       "      <td>13.126460</td>\n",
       "      <td>1244.76</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.055533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Butte</th>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.145583</td>\n",
       "      <td>14</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.398264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178750</td>\n",
       "      <td>0.127667</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>27.727273</td>\n",
       "      <td>1389.20</td>\n",
       "      <td>0.996443</td>\n",
       "      <td>1.130546</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.096562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calaveras</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203345</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.414367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>120.672254</td>\n",
       "      <td>19.746369</td>\n",
       "      <td>10.970205</td>\n",
       "      <td>1068.98</td>\n",
       "      <td>1.032918</td>\n",
       "      <td>1.089488</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           des_percent  perc_lali  food_deserts  urban_des  rural_des  \\\n",
       "County                                                                  \n",
       "Alameda       0.041667   0.018032            15   0.042373   0.000000   \n",
       "Alpine        0.000000   0.266288             0   0.000000   0.000000   \n",
       "Amador        0.000000   0.149514             0   0.000000   0.000000   \n",
       "Butte         0.274510   0.145583            14   0.342105   0.076923   \n",
       "Calaveras     0.000000   0.203345             0   0.000000   0.000000   \n",
       "\n",
       "            n_urban   n_rural  LowIncomeTracts  low_vehicle_tracts  \\\n",
       "County                                                               \n",
       "Alameda    0.983333  0.016667         0.425000            0.147222   \n",
       "Alpine     0.000000  1.000000         0.000000            0.000000   \n",
       "Amador     0.000000  1.000000         0.222222            0.222222   \n",
       "Butte      0.745098  0.254902         0.627451            0.254902   \n",
       "Calaveras  0.200000  0.800000         0.200000            0.100000   \n",
       "\n",
       "           total_housing_units      ...       readm_30_pn  readm_30_stk  \\\n",
       "County                              ...                                   \n",
       "Alameda               0.360954      ...          0.178778      0.134500   \n",
       "Alpine                0.422979      ...          0.000000      0.000000   \n",
       "Amador                0.382479      ...          0.157000      0.114000   \n",
       "Butte                 0.398264      ...          0.178750      0.127667   \n",
       "Calaveras             0.414367      ...          0.166000      0.144000   \n",
       "\n",
       "                  std  vaccine_dx    food_dx  opiods_rx_1000  MILK_PRICE10  \\\n",
       "County                                                                       \n",
       "Alameda    619.888748   41.648155  29.597337          557.20      1.122215   \n",
       "Alpine       0.000000    0.000000   0.000000          578.68      1.032918   \n",
       "Amador     199.522197   15.751752  13.126460         1244.76      1.032918   \n",
       "Butte      395.000000   15.909091  27.727273         1389.20      0.996443   \n",
       "Calaveras  120.672254   19.746369  10.970205         1068.98      1.032918   \n",
       "\n",
       "           SODA_PRICE10   FFR07  pct_nonwhite  \n",
       "County                                         \n",
       "Alameda        1.164422  1112.0      0.483413  \n",
       "Alpine         1.089488     2.0      0.310400  \n",
       "Amador         1.089488    18.0      0.055533  \n",
       "Butte          1.130546   149.0      0.096562  \n",
       "Calaveras      1.089488    27.0      0.060800  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "df[['total_housing_units', 'pop_in_group_housing']] = df[['total_housing_units', 'pop_in_group_housing']].div(df.POP2010, axis=0)\n",
    "df['urban_des'] = df['urban_des'].div(df.n_urban, axis=0)\n",
    "df['rural_des']=df['rural_des'].div(df.n_rural, axis=0)\n",
    "df[['n_rural','n_urban','LowIncomeTracts','low_vehicle_tracts']] = df[['n_rural','n_urban','LowIncomeTracts','low_vehicle_tracts']].div(df.n_tracts, axis=0)\n",
    "\n",
    "#drop= ['POP2010','n_tracts','opiods_greater_than_stateavg','MILK_SODA_PRICE10', 'PCH_FFR_07_12','FFR12']\n",
    "#drop = drop\n",
    "#keep = [c for c in cols if c not in drop]\n",
    "df=df[keep].fillna(0)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "df = order(df,['des_percent','perc_lali']).set_index('County')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs=df[['des_percent','ABR','p_hs_edatt','PC_PHYS_R','unemployment_rate','n_urban','cnty_obesity_pct_adj','perc_lali']]\n",
    "g = sns.pairplot(pairs, diag_kind=\"kde\", kind='reg',\n",
    "                 diag_kws=dict(shade=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resid_sq_error(actual, predicted, n_feats):\n",
    "    rse = (actual-predicted)**2\n",
    "    err_sum = np.sum(rse)\n",
    "    denom = (len(actual)-n_feats) - 1\n",
    "    return np.sqrt(RSEd)/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'des_percent', u'perc_lali', u'food_deserts', u'urban_des',\n",
       "       u'rural_des', u'n_urban', u'n_rural', u'LowIncomeTracts',\n",
       "       u'low_vehicle_tracts', u'total_housing_units', u'pop_in_group_housing',\n",
       "       u'pop_effected', u'state_pop_ratio', u'state_tract_ratio',\n",
       "       u'cnty_obesity_pct_adj', u'cnty_dm_pct_adj', u'cnty_inactive_pct_adj',\n",
       "       u'Adolescent_births', u'ABR', u'p_hs_edatt', u'PC_PHYS_R', u'DENTIST_R',\n",
       "       u'PSYCH_R', u'PCT_WHITE', u'PCT_65OVER', u'PCT_18_64', u'PCT_UNDR18',\n",
       "       u'PCT_UNDER5', u'unemployment_rate', u'n_hospitals', u'mort_30_ami',\n",
       "       u'mort_30_cabg', u'mort_30_copd', u'mort_30_hf', u'mort_30_pn',\n",
       "       u'mort_30_stk', u'readm_30_ami', u'readm_30_cabg', u'readm_30_copd',\n",
       "       u'readm_30_hf', u'readm_30_hip_knee', u'readm_30_hosp_wide',\n",
       "       u'readm_30_pn', u'readm_30_stk', u'std', u'vaccine_dx', u'food_dx',\n",
       "       u'opiods_rx_1000', u'MILK_PRICE10', u'SODA_PRICE10', u'FFR07',\n",
       "       u'pct_nonwhite'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race+age\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>des_percent</td>   <th>  R-squared:         </th> <td>   0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.9420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 08 Aug 2016</td> <th>  Prob (F-statistic):</th>  <td> 0.396</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:29:52</td>     <th>  Log-Likelihood:    </th> <td>  28.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    58</td>      <th>  AIC:               </th> <td>  -51.91</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    55</td>      <th>  BIC:               </th> <td>  -45.73</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>   -0.0235</td> <td>    0.139</td> <td>   -0.169</td> <td> 0.867</td> <td>   -0.303     0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PCT_65OVER</th> <td>    0.3597</td> <td>    0.721</td> <td>    0.499</td> <td> 0.620</td> <td>   -1.084     1.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PCT_WHITE</th>  <td>    0.1495</td> <td>    0.228</td> <td>    0.655</td> <td> 0.515</td> <td>   -0.308     0.607</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22.305</td> <th>  Durbin-Watson:     </th> <td>   1.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  33.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.379</td> <th>  Prob(JB):          </th> <td>5.32e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.502</td> <th>  Cond. No.          </th> <td>    47.6</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            des_percent   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                 -0.002\n",
       "Method:                 Least Squares   F-statistic:                    0.9420\n",
       "Date:                Mon, 08 Aug 2016   Prob (F-statistic):              0.396\n",
       "Time:                        16:29:52   Log-Likelihood:                 28.957\n",
       "No. Observations:                  58   AIC:                            -51.91\n",
       "Df Residuals:                      55   BIC:                            -45.73\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0235      0.139     -0.169      0.867        -0.303     0.256\n",
       "PCT_65OVER     0.3597      0.721      0.499      0.620        -1.084     1.804\n",
       "PCT_WHITE      0.1495      0.228      0.655      0.515        -0.308     0.607\n",
       "==============================================================================\n",
       "Omnibus:                       22.305   Durbin-Watson:                   1.814\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               33.498\n",
       "Skew:                           1.379   Prob(JB):                     5.32e-08\n",
       "Kurtosis:                       5.502   Cond. No.                         47.6\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = df.columns.tolist()\n",
    "y = df['des_percent']\n",
    "X = df[[age[]]+['PCT_WHITE']]\n",
    "Xc = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y,Xc)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Statsmodels summary Outputs\n",
    "Looking for a high R2, F-Statistic, low Prop(F-Statistic), low pvalues, low Condition No, low RSE.\n",
    "Things to note when adding new predictors:\n",
    "* A low pvalue and a t-value far from zero indicates a strong predictor.\n",
    "* whether the addition of the variable resulted in all predictors and intercept have similar coefs\n",
    "* whether values of R2 and adj-R2 are similar to the previous model\n",
    "* changes in magnitude and direction of existing coefs\n",
    "* decreases in the F-statistic - predictor did not benefit model\n",
    "* increases in RSE\n",
    "* Condition No reflect the level of multicollinearity. The higher the Condition No, the more unstable the results.\n",
    "\n",
    "## Residual Plots\n",
    "\n",
    "Residual plots: The residual is the difference between the actual value and the predicted value of the output variable. A plot of the residuals plotted against the predictor variable should be randomly (normally with mean zero and constant variance) distributed and shouldn't have an identi able shape. If the residual follows a characteristic curve, then it means that these errors can be predicted, which means something is wrong with the model and there is a scope for improvement. The error term for a fair estimate should be random and that's why if the residual plot shows a characteristic pattern there is a reason to improvise upon the model.\n",
    "\n",
    "Ideally, the points in a residual plot should be:\n",
    "• Symmetrically distributed or tending to cluster towards the middle of the plot\n",
    "• There are no clear patterns in the plot\n",
    "The non-ideal residual plots having characteristic shapes are observed because of one of the following reasons:\n",
    "• Non-linear relationship\n",
    "• Presence of outliers\n",
    "• Very large Y-axis datapoint\n",
    "##### Bad Residual plot? - Funnel shape: log or sqrt, outliers: remove outliers\n",
    "\n",
    "## High leverage points: \n",
    "In contrast to outliers, which have high values for the output variables, the high leverage points have a very high value of predictor variables. Such a value can distort the model.\n",
    "\n",
    "## Multicollinearity\n",
    "##### correlation matirix, \n",
    "\n",
    "\n",
    "## Gradient Decsent \n",
    "Attempts to optimize a reggression to obtain the best possible fit. This is accomplished by comparing various slope values for each observation in the dataset using a defined cost function, the residual squared error. The function included in this notebook is initialized with random datapoints, which are then changed until the decent's improvement to the models fit is negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha=0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "    \n",
    "def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path\n",
    "\n",
    "alpha = 0.02\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"These are our final standardized coefficients: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstandardizing coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "variables = X.columns.tolist()\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing feature importance using standardiazed coefficients\n",
    "\n",
    "#### First let's look at the coefficients obtained without standardization below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "all_vars = predictors.append(target)\n",
    "\n",
    "predictors = X.columns.tolist()\n",
    "\n",
    "dataset = df[predictors]#[all_vars]\n",
    "linear_regression = LinearRegression(normalize=False,fit_intercept=True)\n",
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization,linear_regression)\n",
    "linear_regression.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,linear_regression.coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the coefficients following standardization:\n",
    "\n",
    "\"Having all the predictors on a similar scale now, we can easily provide a more realistic interpretation of each coefcient. Clearly, it appears that a unit change has more impact when it involves the variables POP2010, Adolescent_births, PCT_UNDER5, OHU2010, readm_30_stk, num_urban, FFR12, PCT_UNDR18, n_hospitals, PCT_65OVER, PCT_18_64, readm_30_copd, and NUMGQTRS. The order of the features below show their relevancy when standardized for predicting the number of food deserts present within a county\" (Linear Regression, p.83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "observations = len(dataset)\n",
    "  variables = dataset.columns\n",
    "  standardization = StandardScaler()\n",
    "  Xst = standardization.fit_transform(X)\n",
    "  original_means = standardization.mean_\n",
    "  originanal_stds = standardization.std_\n",
    "  Xst = np.column_stack((Xst,np.ones(observations)))\n",
    "  y  = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/desert/desert_workspace/desert_data/clean_data.csv')\n",
    "# df = df.drop('Unnamed: 0', axis=1)\n",
    "# df = df.fillna(0)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['pop2010_in_des','des_percent']].describe()\n",
    "df['at_risk'] = (df['pop2010_in_des'] >= 21217) & (df['des_percent']>=0.079662)\n",
    "df['at_risk'] =df['at_risk'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols=df.columns.tolist()\n",
    "cols = [col for col in cols if col != 'County']\n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev','at_risk','MILK_SODA_PRICE10','n_rural','n_urban','Adolescent_births','PCH_FFR_07_12']\n",
    "target = 'n_food_des' \n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "#predictors = ['num_tracts','opiods_rx_1000','SODA_PRICE10']\n",
    "X = df[predictors].values\n",
    "y = np.array(df[target])#.reshape(-1,1)\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X, y)\n",
    "\n",
    "print 'sse\\n', stats.sse(ols, X, y)\n",
    "print 'r2\\n', stats.adj_r2_score(ols, X, y)\n",
    "print 'standard error of beta coeffs\\n', stats.coef_se(ols, X, y)\n",
    "print 't values of beta coeffs\\n',stats.coef_tval(ols, X, y)\n",
    "print 'p values of beta coeffs\\n', stats.coef_pval(ols, X, y)\n",
    "print 'f stat of beta coeffs\\n', stats.f_stat(ols, X, y)\n",
    "\n",
    "xlabels=df[predictors].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.summary(ols, X, y, xlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = smf.ols(formula='n_food_des ~ num_tracts+opiods_rx_1000+SODA_PRICE10', data=df).fit()\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from regressors import plots\n",
    "plots.plot_residuals(ols, X, y, r_type='standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots.plot_qq(ols, X, y, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = X[:,0:6]\n",
    "x_scaled = scaler.fit_transform(X)\n",
    "pcomp = decomposition.PCA()\n",
    "pcomp.fit(x_scaled)\n",
    "np.shape(X)#[0]\n",
    "#X\n",
    "plots.plot_pca_pairs(pcomp, X, y, n_components=min(np.shape(X)), cmap=\"GnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots.plot_scree(pcomp, required_var=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from regressors import regressors\n",
    "pcr = regressors.PCR(n_components=min(np.shape(X)), regression_type='ols')\n",
    "pcr.fit(X, y)\n",
    "\n",
    "# The fitted scaler, pca, and scaler models can be accessed:\n",
    "scaler, pca, regression = (pcr.scaler, pcr.prcomp, pcr.regression)\n",
    "\n",
    "# You could then make various plots, such as pca_pairs_plot(), and\n",
    "# plot_residuals() with these fitted model from PCR.\n",
    "plots.plot_pca_pairs(pca, X, y, n_components=min(np.shape(X)), cmap=\"GnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcr = regressors.PCR(n_components=min(np.shape(X)), regression_type='ols')\n",
    "pcr.fit(X, y)\n",
    "pcr.beta_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X\n",
    "df[predictors].iloc[:,0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcr.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mean(self):\n",
    "    \"\"\"Computes the mean of a PMF.\n",
    "    Returns:\n",
    "        float mean\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    for x, p in self.d.items():\n",
    "        mean += p * x\n",
    "    return mean\n",
    "\n",
    "def Cov(xs, ys, meanx=None, meany=None):\n",
    "    \"\"\"Computes Cov(X, Y).\n",
    "    Args:\n",
    "        xs: sequence of values\n",
    "        ys: sequence of values\n",
    "        meanx: optional float mean of xs\n",
    "        meany: optional float mean of ys\n",
    "    Returns:\n",
    "        Cov(X, Y)\n",
    "    \"\"\"\n",
    "    if meanx is None:\n",
    "        meanx = np.mean(xs)\n",
    "    if meany is None:\n",
    "        meany = np.mean(ys)\n",
    "\n",
    "    cov = np.dot(np.asarray(xs)-meanx, np.asarray(ys)-meany) / len(xs)\n",
    "    return cov\n",
    "\n",
    "def Residuals(xs, ys, inter, slope):\n",
    "    \"\"\"Computes residuals for a linear fit with parameters inter and slope.\n",
    "    Args:\n",
    "        xs: independent variable\n",
    "        ys: dependent variable\n",
    "        inter: float intercept\n",
    "        slope: float slope\n",
    "    Returns:\n",
    "        list of residuals\n",
    "    \"\"\"\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "    res = ys - (inter + slope * xs)\n",
    "    return res\n",
    "\n",
    "def Var(xs, mu=None, ddof=0):\n",
    "    \"\"\"Computes variance.\n",
    "    xs: sequence of values\n",
    "    mu: option known mean\n",
    "    ddof: delta degrees of freedom\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    xs = np.asarray(xs)\n",
    "\n",
    "    if mu is None:\n",
    "        mu = xs.mean()\n",
    "\n",
    "    ds = xs - mu\n",
    "    return np.dot(ds, ds) / (len(xs) - ddof)\n",
    "\n",
    "def CoefDetermination(ys, res):\n",
    "    \"\"\"Computes the coefficient of determination (R^2) for given residuals.\n",
    "    Args:\n",
    "        ys: dependent variable\n",
    "        res: residuals\n",
    "        \n",
    "    Returns:\n",
    "        float coefficient of determination\n",
    "    \"\"\"\n",
    "    return 1 - Var(res) / Var(ys)\n",
    "\n",
    "\n",
    "def MeanVar(xs, ddof=0):\n",
    "    \"\"\"Computes mean and variance.\n",
    "    Based on http://stackoverflow.com/questions/19391149/\n",
    "    numpy-mean-and-variance-from-single-function\n",
    "    xs: sequence of values\n",
    "    ddof: delta degrees of freedom\n",
    "    \n",
    "    returns: pair of float, mean and var\n",
    "    \"\"\"\n",
    "    xs = np.asarray(xs)\n",
    "    xbar = xs.mean()\n",
    "    s2 = Var(xs, xbar, ddof)\n",
    "    return xbar, s2\n",
    "\n",
    "def LeastSquares(xs, ys):\n",
    "    \"\"\"Computes a linear least squares fit for ys as a function of xs.\n",
    "    Args:\n",
    "        xs: sequence of values\n",
    "        ys: sequence of values\n",
    "    Returns:\n",
    "        tuple of (intercept, slope)\n",
    "    \"\"\"\n",
    "    meanx, varx = MeanVar(xs)\n",
    "    meany = Mean(ys)\n",
    "\n",
    "    slope = Cov(xs, ys, meanx, meany) / varx\n",
    "    inter = meany - slope * meanx\n",
    "\n",
    "    return inter, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Get data\n",
    "X1 = df[x1cols].values\n",
    "X2 = df[x2cols].values\n",
    "\n",
    "# Define \"classifiers\" to be used\n",
    "classifiers = {\n",
    "    \"Empirical Covariance\": EllipticEnvelope(support_fraction=1.,\n",
    "                                             contamination=0.261),\n",
    "    \"Robust Covariance (Minimum Covariance Determinant)\":\n",
    "    EllipticEnvelope(contamination=0.261),\n",
    "    \"OCSVM\": OneClassSVM(nu=0.261, gamma=0.05)}\n",
    "colors = ['m', 'g', 'b']\n",
    "legend1 = {}\n",
    "legend2 = {}\n",
    "\n",
    "# Learn a frontier for outlier detection with several classifiers\n",
    "xx1, yy1 = np.meshgrid(np.linspace((np.min(X1[0]))-2, (np.max(X1[0])+2)), \n",
    "                       np.linspace((np.min(X1[-1]))-2, (np.max(X1[-1])+2)))\n",
    "xx2, yy2 = np.meshgrid(np.linspace((np.min(X2[0]))-2, (np.max(X2[0])+2)), \n",
    "                       np.linspace((np.min(X2[-1]))-2, (np.max(X2[-1])+2)))\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    plt.figure(1)\n",
    "    clf.fit(X1)\n",
    "    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "    Z1 = Z1.reshape(xx1.shape)\n",
    "    legend1[clf_name] = plt.contour(\n",
    "        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n",
    "    plt.figure(2)\n",
    "    clf.fit(X2)\n",
    "    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "    Z2 = Z2.reshape(xx2.shape)\n",
    "    legend2[clf_name] = plt.contour(\n",
    "        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n",
    "\n",
    "legend1_values_list = list( legend1.values() )\n",
    "legend1_keys_list = list( legend1.keys() )\n",
    "\n",
    "# Plot the results (= shape of the data points cloud)\n",
    "plt.figure(1)  # two clusters\n",
    "plt.title(\"Outlier detection on n_tracts\")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='black')\n",
    "bbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"->\")\n",
    "# plt.annotate(\"several confounded points\", xy=(24, 19),\n",
    "#              xycoords=\"data\", textcoords=\"data\",\n",
    "#              xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\n",
    "#plt.xlim((xx1.min(), xx1.max()))\n",
    "#plt.ylim((yy1.min(), yy1.max()))\n",
    "plt.legend((legend1_values_list[0].collections[0],\n",
    "            legend1_values_list[1].collections[0],\n",
    "            legend1_values_list[2].collections[0]),\n",
    "           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"n_food_des\")\n",
    "plt.xlabel(\"n_tracts\")\n",
    "\n",
    "legend2_values_list = list( legend2.values() )\n",
    "legend2_keys_list = list( legend2.keys() )\n",
    "\n",
    "plt.figure(2)  # \"banana\" shape\n",
    "plt.title(\"Outlier detection on n_urban \")\n",
    "plt.scatter(X2[:, 0], X2[:, 1], color='black')\n",
    "plt.xlim((-1, 500))\n",
    "plt.ylim((-1, 40))\n",
    "#plt.xlim((xx2.min(), xx2.max()))\n",
    "#plt.ylim((yy2.min(), yy2.max()))\n",
    "plt.legend((legend2_values_list[0].collections[0],\n",
    "            legend2_values_list[1].collections[0],\n",
    "            legend2_values_list[2].collections[0]),\n",
    "           (legend2_values_list[0], legend2_values_list[1], legend2_values_list[2]),\n",
    "           loc=\"upper center\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=12))\n",
    "plt.ylabel(\"n_food_des\")\n",
    "plt.xlabel(\"n_urban\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_outlier(value, p25, p75):\n",
    "    \"\"\"Check if value is an outlier\n",
    "    \"\"\"\n",
    "    lower = p25 - 1.5 * (p75 - p25)\n",
    "    upper = p75 + 1.5 * (p75 - p25)\n",
    "    return value <= lower or value >= upper\n",
    " \n",
    "\n",
    "def get_indices_of_outliers(values):\n",
    "    \"\"\"Get outlier indices (if any)\n",
    "    \"\"\"\n",
    "    p25 = np.percentile(values, 25)\n",
    "    p75 = np.percentile(values, 75)\n",
    "     \n",
    "    indices_of_outliers = []\n",
    "    for ind, value in enumerate(values):\n",
    "        if is_outlier(value, p25, p75):\n",
    "            indices_of_outliers.append(ind)\n",
    "    return indices_of_outliers\n",
    " \n",
    "x1cols = [col for col in cols if col in ['num_tracts', 'n_food_des']]\n",
    "x2cols=[col for col in cols if col in [ 'n_food_des', 'HIV']]\n",
    "x2cols.reverse()\n",
    "x2cols\n",
    "X1 = df[x1cols]\n",
    "X2 = df[x2cols]\n",
    "\n",
    "feat=X2.iloc[:,0]\n",
    "values=X2.iloc[:,0].values\n",
    "y=X2.iloc[:,1].values\n",
    "indices_of_outliers = get_indices_of_outliers(feat)\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(feat, 'b-', label=x2cols[0])\n",
    "ax.plot(indices_of_outliers, values[indices_of_outliers],'ro',markersize = 7,label='outliers')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(X1.iloc[:,1], 'b-')#, label=x1cols[0])\n",
    "plt.scatter(X2.iloc[:,0].values, X2.iloc[:,1].values)\n",
    "plt.plot( values[indices_of_outliers],y[indices_of_outliers],'ro',markersize = 7,label='outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2.iloc[indices_of_outliers,:].describe()\n",
    "\n",
    "#X2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "dataset = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "x1=dataset.columns.tolist()\n",
    "x1=[x1[i] for i in range(len(x1)) if i in [8, 10]]\n",
    "x2=dataset.columns.tolist()\n",
    "x2=[x2[i] for i in range(len(x2)) if i in [5, 12]]\n",
    "#sns.pairplot(dataset[x2])\n",
    "dataset[x2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=['num_tracts', 'n_urban', 'n_rural', 'urban_des'])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[5:10])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[10:15])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[15:20])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[20:25])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[25:30])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[30:35])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[35:40])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[40:45])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[45:50])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[50:55])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[55:60])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[60:65])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[65:70])\n",
    "sns.pairplot(df,y_vars=['n_food_des'],x_vars=cols[70:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def lin_poly_compare(X,y,xname):\n",
    "    x_copy = X.copy()\n",
    "    lr = LinearRegression()\n",
    "    pr = LinearRegression()\n",
    "    cr = LinearRegression()\n",
    "    quadratic = PolynomialFeatures(degree=2)\n",
    "    cubic = PolynomialFeatures(degree=3)\n",
    "    X_quad = quadratic.fit_transform(X)\n",
    "    X_cubic = cubic.fit_transform(x_copy)\n",
    "    lr.fit(X, y)\n",
    "    X_fit = np.arange(np.min(X),np.max(X),np.std(X))[:, np.newaxis]\n",
    "    y_lin_fit = lr.predict(X_fit)\n",
    "    pr.fit(X_quad, y)\n",
    "    y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))\n",
    "    cr.fit(X_cubic, y)\n",
    "    y_cubic_fit = cr.predict(cubic.fit_transform(X_fit))\n",
    "    plt.scatter(X, y, label='training points')\n",
    "    plt.plot(X_fit, y_lin_fit, label='linear fit', linestyle='--')\n",
    "    plt.plot(X_fit, y_quad_fit, label='quadratic fit')\n",
    "    plt.plot(X_fit, y_quad_fit, label='cubic fit',linestyle=':')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(xname)\n",
    "    plt.ylabel(\"n_food_des\")\n",
    "    plt.title('Comparing Linear & Polynomial fit')\n",
    "    plt.show()\n",
    "    y_lin_pred = lr.predict(X)\n",
    "    y_quad_pred = pr.predict(X_quad)\n",
    "    y_cubic_pred = cr.predict(X_cubic)\n",
    "    lin_mse, quad_mse, cubic_mse = mean_squared_error(y, y_lin_pred),mean_squared_error(y, y_quad_pred),mean_squared_error(y, y_cubic_pred)\n",
    "    lin_r2, quad_r2, cubic_r2 = r2_score(y, y_lin_pred),r2_score(y, y_quad_pred),r2_score(y, y_cubic_pred)\n",
    "    print('Training MSE linear: %.3f, quadratic: %.3f, cubic: %.3f' % (mean_squared_error(y, y_lin_pred),mean_squared_error(y, y_quad_pred),mean_squared_error(y, y_cubic_pred)))\n",
    "    print('Training R^2 linear: %.3f, quadratic: %.3f, cubic: %.3f' % ( r2_score(y, y_lin_pred),r2_score(y, y_quad_pred),r2_score(y, y_cubic_pred)))\n",
    "    r2lst = [lin_r2, quad_r2, cubic_r2]\n",
    "    mselst= [lin_mse, quad_mse, cubic_mse]\n",
    "    if np.min(mselst) == lin_mse:\n",
    "        print 'Linear'\n",
    "    if np.min(mselst) == quad_mse:\n",
    "        print 'Quadratic'\n",
    "    if np.min(mselst) == cubic_mse:\n",
    "        print 'Cubic'\n",
    "    if np.max(r2lst)==lin_r2:\n",
    "        print 'LINEAR FITS BETTER'\n",
    "        return 'linear'\n",
    "    if np.max(r2lst)==quad_r2:\n",
    "        return 'quadratic'\n",
    "    else:\n",
    "        return 'cubic'\n",
    "\n",
    "def RemoveOutliers(frame, m=3):\n",
    "    new_data = pd.DataFrame(frame)\n",
    "    clean_data = new_data[np.abs(new_data-new_data.mean())<=(m*new_data.std())]\n",
    "    return clean_data.dropna(axis=0)\n",
    "        \n",
    "data = df.copy(deep=False)\n",
    "lin_feats = {}\n",
    "lin_feats_wo_outliers = {}\n",
    "for i in range(len(cols)):\n",
    "#     if i > 10:\n",
    "#         break\n",
    "    if cols[i] != 'n_food_des':\n",
    "        name = cols[i]\n",
    "        lin_feats[name] = lin_poly_compare(np.array(df[cols[i]]).reshape(-1,1),np.array(df['n_food_des']).reshape(-1,1),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_feats\n",
    "# linear = ['Rubella','high_food_des_prev','opiods_greater_than_stateavg' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations = len(df)\n",
    "cols = [col for col in cols if col != 'County']\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev','at_risk','MILK_SODA_PRICE10','n_rural','n_urban','Adolescent_births','PCH_FFR_07_12']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "X = df[predictors].values\n",
    "Y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from minepy import MINE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "np.random.seed(0)\n",
    "ranks = {}\n",
    " \n",
    "def rank_to_dict(ranks, predictors, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(predictors, ranks ))\n",
    " \n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X, Y)\n",
    "ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), predictors)\n",
    " \n",
    "ridge = Ridge(alpha=7)\n",
    "ridge.fit(X, Y)\n",
    "ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), predictors)\n",
    " \n",
    "lasso = Lasso(alpha=.05)\n",
    "lasso.fit(X, Y)\n",
    "ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), predictors)\n",
    " \n",
    "rlasso = RandomizedLasso(alpha=0.04)\n",
    "rlasso.fit(X, Y)\n",
    "ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), predictors)\n",
    " \n",
    "#stop the search when 5 features are left (they will get equal scores)\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X,Y)\n",
    "ranks[\"RFE\"] = rank_to_dict(map(float, rfe.ranking_), predictors, order=-1)\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X,Y)\n",
    "ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, predictors)\n",
    " \n",
    "f, pval  = f_regression(X, Y, center=True)\n",
    "ranks[\"Corr.\"] = rank_to_dict(f, predictors)\n",
    " \n",
    "mine = MINE()\n",
    "mic_scores = []\n",
    "for i in range(X.shape[1]):\n",
    "    mine.compute_score(X[:,i], Y)\n",
    "    m = mine.mic()\n",
    "    mic_scores.append(m)\n",
    "\n",
    "ranks[\"MIC\"] = rank_to_dict(mic_scores, predictors) \n",
    " \n",
    "r = {}\n",
    "for name in predictors:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print \"\\t%s\" % \"\\t\".join(methods)\n",
    "for name in predictors:\n",
    "    print \"%s\\t%s\" % (name, \"\\t\".join(map(str, [ranks[method][name] for method in methods])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(ranks)\n",
    "feature_df.describe()\n",
    "\n",
    "feature_df[feature_df['Corr.']>= 0.15] # shows the predictors with the strongest correlation with the target variable\n",
    "feature_df[feature_df['Linear reg']>= 0.07]# quatifies the effect of the most influential predictor_vars on target variable \n",
    "feature_df[feature_df['Lasso'] >= 0.2] # identifies top features \n",
    "feature_df[feature_df['MIC'] >= 0.505] # shows the most mutal dependent predictors based on optimal binning. MIC measures mutual dependence between variables (predictor & target)\n",
    "feature_df[feature_df['RF'] > 0.03] # imp predictors based on impurity reduction. May dismiss important colinear vars. May cause misintepretation of predictors.\n",
    "feature_df[feature_df['Ridge'] >= 0.16]\n",
    "#feature_df['Ridge']# helps to interpret ALL predictive features\n",
    "feature_df[feature_df['Ridge'] > 0].sort_values(by='Ridge').tail(10) # helps to interpret predictive features, coefficients are stable\n",
    "feature_df[feature_df['Ridge'] == 0] # identifies non-predictive features\n",
    "feature_df[feature_df['Stability'] >= .47] # shows the most stable features for model\n",
    "w_outliers =feature_df[feature_df['Mean'] >= .28] # aggregating all methods - useful for both selection & interpretation.\n",
    "w_outliers =w_outliers.index\n",
    "w_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RemoveOutliers(frame, m=3):\n",
    "    new_data = pd.DataFrame(frame)\n",
    "    clean_data = new_data[np.abs(new_data-new_data.mean())<=(m*new_data.std())]\n",
    "    return clean_data.dropna(axis=0)\n",
    "\n",
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev','at_risk','MILK_SODA_PRICE10','n_rural','n_urban','Adolescent_births','PCH_FFR_07_12']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "X = df[predictors]\n",
    "X = order(X, [target])\n",
    "data=X\n",
    "pred_vars = X.columns.tolist()[1:]\n",
    "target_var = X.columns.tolist()[0]\n",
    "\n",
    "wo_outliers = RemoveOutliers(data)\n",
    "wo_outliers\n",
    "\n",
    "np.random.seed(0)\n",
    "ranks = {}\n",
    "X = wo_outliers[pred_vars]\n",
    "Y = wo_outliers[target_var].values\n",
    " \n",
    "def rank_to_dict(ranks, pred_vars, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(pred_vars, ranks ))\n",
    "\n",
    "def assess_predictors(X_vars, target_vars, names):\n",
    "    lr = LinearRegression(normalize=True)\n",
    "    lr.fit(X_vars, target_vars)\n",
    "    ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    "    ridge = Ridge(alpha=7)\n",
    "    ridge.fit(X_vars, target_vars)\n",
    "    ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    "    lasso = Lasso(alpha=.05)\n",
    "    lasso.fit(X_vars, target_vars)\n",
    "    ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    "    rlasso = RandomizedLasso(alpha=0.04)\n",
    "    rlasso.fit(X_vars, target_vars)\n",
    "    ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n",
    "    rfe = RFE(lr, n_features_to_select=5)\n",
    "    rfe.fit(X_vars,target_vars)\n",
    "    ranks[\"RFE\"] = rank_to_dict(map(float, rfe.ranking_), names, order=-1)\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_vars,target_vars)\n",
    "    ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    "    f, pval  = f_regression(X_vars, target_vars, center=True)\n",
    "    #ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    "#     mine = MINE()\n",
    "#     mic_scores = []\n",
    "#     for i in range(X_vars.shape[1]):\n",
    "#         mine.compute_score(X_vars[:,i], target_vars)\n",
    "#         m = mine.mic()\n",
    "#         mic_scores.append(m)\n",
    "#     ranks[\"MIC\"] = rank_to_dict(mic_scores, names) \n",
    "    r = {}\n",
    "    for name in names:\n",
    "        r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n",
    "    methods = sorted(ranks.keys())\n",
    "    ranks[\"Mean\"] = r\n",
    "    methods.append(\"Mean\")\n",
    "    print \"\\t%s\" % \"\\t\".join(methods)\n",
    "    for name in names:\n",
    "        print \"%s\\t%s\" % (name, \"\\t\".join(map(str, [ranks[method][name] for method in methods])))\n",
    "    return pd.DataFrame(ranks)\n",
    "\n",
    "\n",
    "wo_outlier_feats=assess_predictors(X, Y, X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wo_outlier_feats.describe()\n",
    "wo_outlier_feats[wo_outlier_feats['Linear reg']>= 0.01]# quatifies the effect of the most influential predictor_vars on target variable \n",
    "wo_outlier_feats[wo_outlier_feats['Lasso'] >= 0.115] # identifies top features \n",
    "wo_outlier_feats[wo_outlier_feats['RF'] > 0.03] # imp predictors based on impurity reduction. May dismiss important colinear vars. May cause misintepretation of predictors.\n",
    "wo_outlier_feats[wo_outlier_feats['Ridge'] >= 0.27]\n",
    "wo_outlier_feats['Ridge']# helps to interpret ALL predictive features\n",
    "wo_outlier_feats[wo_outlier_feats['Ridge'] > 0].sort_values(by='Ridge').tail(10) # helps to interpret predictive features, coefficients are stable\n",
    "wo_outlier_feats[wo_outlier_feats['Ridge'] == 0] # identifies non-predictive features\n",
    "wo_outlier_feats[wo_outlier_feats['Stability'] >= .43] # shows the most stable features for model\n",
    "wo_outlier_feats[wo_outlier_feats['Mean'] >= .285] # aggregating all methods - useful for both selection & interpretation.\n",
    "wo_outlier=wo_outlier_feats[wo_outlier_feats['Mean'] >= .285].index\n",
    "wo_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wo_outlier)\n",
    "len(w_outliers)\n",
    "sns.pairplot(wo_outliers[list(wo_outlier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[list(w_outliers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = df[[list(w_outliers), 'n_food_des']]\n",
    "lst=list(w_outliers)\n",
    "lst.append('n_food_des')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst2=list(wo_outlier)\n",
    "lst2.append('n_food_des')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = wo_outliers[lst2].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = wo_outliers[lst2[-1]]\n",
    "X = wo_outliers[lst2[:-1]]\n",
    "X = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y,X)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst\n",
    "corrmat = df[lst].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df[lst[-1]]\n",
    "X = df[lst[:-1]]\n",
    "X = sm.add_constant(X)\n",
    "linear_regression = sm.OLS(y,X)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model\n",
    "\n",
    "lr_model=forward_selected(df[predictors], 'n_food_des')\n",
    "#col for col in df.columns.tolist() in col in predictors or == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Parameters: ', lr_model.params)\n",
    "# print('Standard errors: ', lr_model.bse)\n",
    "# print('Predicted values: ', lr_model.predict())\n",
    "xcols =[col for col in predictors if col != 'n_food_des']\n",
    "X=df[xcols]\n",
    "norm_x = df[xcols].values\n",
    "for i, name in enumerate(X):\n",
    "    if name == \"const\":\n",
    "        continue\n",
    "    norm_x[:,i] = X[name]/np.linalg.norm(X[name])\n",
    "norm_xtx = np.dot(norm_x.T,norm_x)\n",
    "\n",
    "eigs = np.linalg.eigvals(norm_xtx)\n",
    "condition_number = np.sqrt(eigs.max() / eigs.min())\n",
    "print(condition_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infl = lr_model.get_influence()\n",
    "\n",
    "2./len(X)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infl.summary_frame()[infl.summary_frame().filter(regex=\"dfb\") < 0.2626128657194451].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_no_target = df[predictors]\n",
    "data = df.copy(deep=False)\n",
    "data_no_target\n",
    "response = data[target].values\n",
    "#data.transform(data_no_target.T.to_dict())\n",
    "LR = LinearRegression().fit(data_no_target, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' + '.join([format(LR.intercept_, '0.2f')] + map(lambda (f,c): \"(%0.2f %s)\" % (c, f), zip(data_no_target.columns, LR.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(data_no_target)\n",
    "np.shape(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingErrs = abs(LR.predict(df[predictors].values - response.reshape(58,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.percentile(trainingErrs, [75, 90, 95, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlierIdx = trainingErrs >= np.percentile(trainingErrs, 95)\n",
    "\n",
    "for i in data_no_target.columns:\n",
    "    plt.scatter(df[i], df.n_food_des, c='b', marker='s')\n",
    "    plt.scatter(df[i][outlierIdx], df.n_food_des[outlierIdx], c='r', marker='s')\n",
    "    plt.show()\n",
    "# plt.scatter(df.mort_30_copd, df.n_food_des, c='b', marker='s')\n",
    "# plt.scatter(df.mort_30_copd[outlierIdx], df.n_food_des[outlierIdx], c='r', marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errs = abs(LR.predict(df[predictors].values - response.reshape(58,1)))\n",
    "plt.hist(errs, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='mort_30_copd', y='n_food_des', data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Food Desert Variable definition reminders:\n",
    "HUNVFlag - Vehicle access, tract with low rate (aggregated over county)   \n",
    "NUMGQTRS - Group quarters, tract population residing in, number (summed over county)   \n",
    "OHU2010 - Housing units, total (summed over county)  \n",
    "POP2010 - total population in 2010 (summed over county)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def draw_histograms(frame, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(16, 12))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        kde = stats.gaussian_kde(frame[var_name])\n",
    "        xx = np.linspace(np.min(frame[var_name]), np.max(frame[var_name]), 1000)\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        frame[var_name].hist(bins=10,ax=ax, normed=True, alpha=0.3)\n",
    "        ax.plot(xx, kde(xx))\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(df, cols[:20], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[20:40], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[40:60], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[60:], 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Normality\n",
    "normaltest returns a 2-tuple of the chi-squared statistic, and the associated p-value. Given the null hypothesis that x came from a normal distribution, the p-value represents the probability that a chi-squared statistic that large (or larger) would be seen.\n",
    "\n",
    "If the p-val is very small, it means it is unlikely that the data came from a normal distribution. (http://stackoverflow.com/questions/12838993/scipy-normaltest-how-is-it-used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xcols=X.columns.tolist()\n",
    "# xcols[0]\n",
    "\n",
    "def assess_normality_of_feats(frame, var):\n",
    "    return stats.normaltest(frame[var])[-1] >= 0.05\n",
    "\n",
    "def obtain_normal_feats(frame, var_lst):\n",
    "    norm = []\n",
    "    for i in var_lst:\n",
    "        #print i, assess_normality_of_feats(frame, i)\n",
    "        if assess_normality_of_feats(frame, i) == True:\n",
    "            norm += [i]\n",
    "    return norm\n",
    "    \n",
    "#norm_no_mc = ['Chlamydia','PCT_65OVER','PCT_HSPNC','PCT_OTHER','PCT_UNDER5','PCT_UNDR18','PCT_WHITE','cnty_dm_pct_adj','cnty_inactive_pct_adj','opiods_rx_1000','readm_30_cabg']\n",
    "\n",
    "norm_cols = obtain_normal_feats(frame=df, var_lst=cols)\n",
    "norm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "observations = len(df)\n",
    "cols = [col for col in cols if col != 'County']\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "X = df[predictors]\n",
    "y  = df[target].values\n",
    "\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.std_\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha=0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "    \n",
    "def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path\n",
    "\n",
    "alpha = 0.02\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"These are our final standardized coefficients: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstandardizing coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "variables = X.columns.tolist()\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing feature importance using standardiazed coefficients\n",
    "\n",
    "#### First let's look at the coefficients obtained without standardization below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "all_vars = predictors.append(target)\n",
    "\n",
    "predictors = X.columns.tolist()\n",
    "\n",
    "dataset = df[predictors]#[all_vars]\n",
    "linear_regression = LinearRegression(normalize=False,fit_intercept=True)\n",
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization,linear_regression)\n",
    "linear_regression.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,linear_regression.coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the coefficients following standardization:\n",
    "\n",
    "\"Having all the predictors on a similar scale now, we can easily provide a more realistic interpretation of each coefcient. Clearly, it appears that a unit change has more impact when it involves the variables POP2010, Adolescent_births, PCT_UNDER5, OHU2010, readm_30_stk, num_urban, FFR12, PCT_UNDR18, n_hospitals, PCT_65OVER, PCT_18_64, readm_30_copd, and NUMGQTRS. The order of the features below show their relevancy when standardized for predicting the number of food deserts present within a county\" (Linear Regression, p.83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "alphas, _, coefs = lars_path(X.values, y, method='lasso', verbose=True)\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dataset)#.values)\n",
    "names = dataset.columns.tolist()\n",
    "  \n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \"\\n + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "  \n",
    "print \"Lasso model: \", pretty_print_linear(lasso.coef_, names, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#good predictors\n",
    "sns.lmplot(x='n_rural', y=target, data=df)\n",
    "sns.lmplot(x='HUNVFlag', y=target, data=df)\n",
    "sns.lmplot(x='HIV', y=target, data=df)\n",
    "sns.lmplot(x='PCT_BLACK', y=target, data=df)\n",
    "sns.lmplot(x='PCT_UNDER5', y=target, data=df)\n",
    "sns.lmplot(x='HIV', y=target, data=df)\n",
    "\n",
    "# bad predictor\n",
    "sns.lmplot(x='FFR12', y=target, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([xx, coefs.T])\n",
    "coefs\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "linear_regression = LinearRegression(normalize=False,fit_intercept=True)\n",
    "\n",
    "def r2_est(X,y):\n",
    "    return r2_score(y,linear_regression.fit(X,y).predict(X))\n",
    "\n",
    "print ('Baseline R2: %0.3f' %  r2_est(X,y))\n",
    "\n",
    "\n",
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X,y) - \\\n",
    "    r2_est(X.values [:,selection],y)) ,dataset.columns[j]))\n",
    "for imp, varname in sorted(r2_impact, reverse=True):\n",
    "    print ('%6.3f %s' %  (imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our dataset by calculating some summary statistics, and making yet another correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_dataframe = df.corr()\n",
    "\n",
    "# compute hierarchical cluster on both rows and columns for correlation matrix and plot heatmap \n",
    "def corr_heatmap(corr_dataframe):\n",
    "    import scipy.cluster.hierarchy as sch\n",
    "    \n",
    "    corr_matrix = np.array(corr_dataframe)\n",
    "    col_names = corr_dataframe.columns\n",
    "    \n",
    "    Y = sch.linkage(corr_matrix, 'single', 'correlation')\n",
    "    Z = sch.dendrogram(Y, color_threshold=0, no_plot=True)['leaves']\n",
    "    corr_matrix = corr_matrix[Z, :]\n",
    "    corr_matrix = corr_matrix[:, Z]\n",
    "    col_names = col_names[Z]\n",
    "    im = plt.imshow(corr_matrix, interpolation='nearest', aspect='auto', cmap='bwr')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(corr_matrix.shape[0]), col_names, rotation='vertical', fontsize=4)\n",
    "    plt.yticks(range(corr_matrix.shape[0]), col_names[::-1], fontsize=4)\n",
    "    \n",
    "# plot\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_high_corr(corr_dataframe, thresh = 0.9):\n",
    "    '''remove predictors with high pairwise correlation'''\n",
    "    abs_corr = np.abs(corr_dataframe).as_matrix() # absolute correlation matrix\n",
    "    col_names = list(corr_dataframe.columns)\n",
    "    \n",
    "    # set up diagonal to 0\n",
    "    np.fill_diagonal(abs_corr, 0)\n",
    "    \n",
    "    print \"Removed predictors (in order): \\n\"\n",
    "    while np.max(abs_corr) >= thresh:\n",
    "        i, j = np.unravel_index(abs_corr.argmax(), abs_corr.shape) # find maximum element\n",
    "        # print abs_corr[i, j]\n",
    "        rdx = which_to_remove(i, j, abs_corr)\n",
    "        # remove corresponding predictor\n",
    "        print col_names.pop(rdx)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 0)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 1)\n",
    "        \n",
    "    return col_names\n",
    "\n",
    "def which_to_remove(i, j, abs_corr):\n",
    "    '''compare two predictors and remove the one with higher abs correlation with other predictors'''\n",
    "    i_absmean = np.mean(abs_corr[i, np.where(abs_corr[i,:] == 0)])\n",
    "    j_absmean = np.mean(abs_corr[j, np.where(abs_corr[j,:] == 0)])\n",
    "    \n",
    "    return i if i_absmean > j_absmean else j\n",
    "\n",
    "# remained predictors\n",
    "col_remained = remove_high_corr(corr_dataframe)\n",
    "data=df[col_remained]\n",
    "corr_dataframe = data.corr()\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix below shows the dataset WITHOUT the highly correlated features identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = corr_dataframe\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas this correlation matrix shows the dataset with all original features (WITH the highly correlated features identified above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection:\n",
    "#### Pearson Correlation Coefficient\n",
    "* The Pearson Correlation measures the linear relationship between X (the predictor variable) and Y (the response variable). Values can range from -1 (which represents a perfect negative correlation) to 1 (which represents a perfect positive correlation). A negative correlation suggests that as the predictor variable, X, increases in value, we are likely to see a decrease in the value of our target variable, Y. A positive correlation suggests that as the predictor variable increases in value, we are likely to see an increase in the target variable as well. A pearson correlation of 0 suggests there is no relationship between the two variables, and they are of no use in predicting the value of one another. \n",
    "\n",
    "______\n",
    "Now that we have an understanding of what the Pearson Correlation Coefficient represents, let's use the functions defined below to determine what features are correlated with 'n_food_des,' or the number of food deserts in California counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "def covariance(variable_1, variable_2, bias=0):\n",
    "    observations = float(len(variable_1))\n",
    "    return np.sum((variable_1 - np.mean(variable_1)) * (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n",
    "\n",
    "def standardize(variable):\n",
    "    return (variable - np.mean(variable)) / np.std(variable)\n",
    "\n",
    "def correlation(var1,var2,bias=0):\n",
    "    return covariance(standardize(var1), standardize(var2),bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "names = predictors\n",
    "X = df[predictors].values\n",
    "Y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_feats_correlation(predictor_vars, target_var, dataset):\n",
    "    feats_corr = {}\n",
    "    for feature in predictor_vars:\n",
    "        #print feature\n",
    "        #print ('Feature Correlation estimation: %0.5f' % (correlation(dataset[feature], dataset[target_var])))\n",
    "        #print '----'*5\n",
    "        feats_corr[feature] = (target, correlation(dataset[feature], dataset[target_var]))\n",
    "    feats_corr = pd.DataFrame(feats_corr).T\n",
    "    feats_corr.columns = ['target_var', 'correlation']\n",
    "    feats_corr['correlation'] = feats_corr['correlation'].astype(float)\n",
    "    return feats_corr\n",
    "        \n",
    "\n",
    "# target = 'pop2010_in_des'\n",
    "# pop_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "# pop_des_corr['correlation'].plot(kind='bar')\n",
    "# plt.show()\n",
    "\n",
    "target = 'n_food_des' \n",
    "n_food_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "\n",
    "n_food_des_corr['correlation'].plot(kind='bar')\n",
    "n_food_des_corr.sort_values('correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_food_des_corr\n",
    "#sns.distplot(df['HIV'])\n",
    "#sns.distplot(standardize(df['HIV']))\n",
    "pearsonr(df['HIV'], df['n_food_des'])\n",
    "\n",
    "def assess_feats_correlation(predictor_vars, target_var, dataset):\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    feats_corr = {}\n",
    "    for feature in predictor_vars:\n",
    "        #print feature\n",
    "        #print ('Feature Correlation estimation: %0.5f' % (correlation(dataset[feature], dataset[target_var])))\n",
    "        #print '----'*5\n",
    "        feats_corr[feature] = (pearsonr(dataset[feature], dataset[target_var]))\n",
    "    feats_corr = pd.DataFrame(feats_corr).T\n",
    "    feats_corr.columns = ['correlation', 'pval']\n",
    "    feats_corr['correlation'] = feats_corr['correlation'].astype(float)\n",
    "    feats_corr['pval'] = feats_corr['pval'].astype(float)\n",
    "    return feats_corr\n",
    "\n",
    "target = 'n_food_des' \n",
    "n_food_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "n_food_des_corr[n_food_des_corr['pval'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keepers = list(n_food_des_corr[n_food_des_corr['pval'] < 0.05].index)\n",
    "len(df[keepers].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for Collinearity\n",
    "\n",
    "#### Definitions:\n",
    "   * Collinearity: Shared variance between two variables\n",
    "   * Multi-collinearity: Shared variance among three or more variables\n",
    "\n",
    "In the previous cells, we assessed each predictor variable's correlation with our target variable using Pearsons Correlation Coefficient. It is important to note however, this approach is only valid when measuring a relationship between two independent variables. \n",
    "\n",
    "Therefore, if we want to include multiple features in our linear regression model, we must assess our features for collinearity. Meaning we must determine whether \"the relation between the variance of the predictor and that of the target is due to unique or shared variance\" (Linear Regression, p72).\n",
    "\n",
    "This can be done by evaluating the partial correlation of each feature we plan to utilize in our model. This value \"represents the exclusive contribution of a variable in predicting the response,\" and will help us avoid misinterpreting collinear features as significant predictors (Linear Regression, p73).\n",
    "\n",
    "Let's asses our features using a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor variables that were shown to have significant correlation with our target \n",
    "# variable independently.\n",
    "X = df[keepers] \n",
    "correlation_matrix = X.corr()\n",
    "print correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    import matplotlib as mpl\n",
    "    variables = data.columns.tolist()\n",
    "    R = np.corrcoef(data, rowvar=0)\n",
    "    R[np.where(np.abs(R)<hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor=False)\n",
    "    plt.tick_params(axis='both', which='both', bottom='off', \\\n",
    "    top='off', left = 'off', right = 'off')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "visualize_correlation_matrix(X, hurdle=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a cut at 0.5 correlation (which translates into a 25% shared variance), the heat map immediately reveals how senior_flu_deaths and PSYCH_R are not so related to other predictors.\n",
    "\n",
    "\"An even more automatic way to detect such associations (and  gure out numerical problems in a matrix inversion) is to use eigenvectors. Explained in layman's terms, eigenvectors are a very smart way to recombine the variance among the variables, creating new features accumulating all the shared variance. Such recombination\n",
    "can be achieved using the NumPy linalg.eig function, resulting in a vector of eigenvalues (representing the amount of recombined variance for each new variable) and eigenvectors (a matrix telling us how the new variables relate to the old ones)\" (Regression Analysis, p.74):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X, rowvar=0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After extracting the eigenvalues, we print them in descending order and look for any element whose value is near to zero or small compared to the others. Near zero values can represent a real problem for normal equations and other optimization methods based on matrix inversion. Small values represent a high but not critical source of multicollinearity. If you spot any of these low values, keep a note of their index in the list.\"(Regression Analysis, p.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_near_zero_eigenvalues(eig_vals):\n",
    "    mc_sources = []\n",
    "    for i in range(len(eigenvalues)):\n",
    "        if eigenvalues[i] <= .1:\n",
    "            mc_sources += [i]\n",
    "    return mc_sources\n",
    "\n",
    "print eigenvalues\n",
    "possible_multicollinear_evals = id_near_zero_eigenvalues(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using their index position in the list of eigenvalues, we can recall their specific vector from eigenvectors, which contains all the variable loadings—that is, the\n",
    "level of association with the original variables. Our eigenvalues dictate that we should investigate the eigenvectors from index 19 to index 34. The functions below use the previously defined id_near_zero_eigenvalues and the eigenvectors to count the number of times a feature was found to have a high amount of collinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exclude_collinear_vars(eig_vecs, explanatory_vars):\n",
    "    good_vars = []\n",
    "    eig_vecs = list(eig_vecs)\n",
    "    not_collinear = [i for i in range(len(eig_vecs)) if eig_vecs[i] <= 0.1 and eig_vecs[i] >= -0.1]\n",
    "    for i in not_collinear:\n",
    "        good_vars += [explanatory_vars[i]]\n",
    "    return good_vars#var for var in variables if not_collinear\n",
    "\n",
    "def id_best_feats(eig_vals):\n",
    "    import itertools, collections\n",
    "    good_vars = []\n",
    "    assess_further = id_near_zero_eigenvalues(eigenvalues)\n",
    "    for i in assess_further:\n",
    "        good_vars += [exclude_collinear_vars(eigenvectors[:,i], variables)]\n",
    "    counter = collections.Counter(itertools.chain(*good_vars))\n",
    "    times_nomc_found = pd.DataFrame([counter.values()], columns=counter.keys()).T\n",
    "    times_nomc_found.columns = ['count']\n",
    "    return times_nomc_found#good_vars\n",
    "\n",
    "num_no_mc_found = id_best_feats(eigenvalues)\n",
    "\n",
    "num_no_mc_found\n",
    "# import itertools, collections\n",
    "# counter = collections.Counter(itertools.chain(*num_no_mc_found))\n",
    "# times_nomc_found = pd.DataFrame([counter.values()], columns=counter.keys()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_nomc_found.columns = ['count']\n",
    "times_nomc_found.describe()\n",
    "least_redundant_feats = times_nomc_found[times_nomc_found >= 10].dropna().index\n",
    "df[least_redundant_feats]\n",
    "feats_inc_y = [val for val in least_redundant_feats]\n",
    "feats_inc_y += ['n_food_des']\n",
    "feats_inc_y\n",
    "corrmat = df[feats_inc_y].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "second_order=PolynomialFeatures(degree=2, interaction_only=False)\n",
    "third_order=PolynomialFeatures(degree=3, interaction_only=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, Y, test_size=0.30, random_state=1)\n",
    "lm = LinearRegression()\n",
    "cv_iterator = KFold(n=len(X_train), n_folds=10, shuffle=True, random_state=101)\n",
    "recursive_selector = RFECV(estimator=lm, step=1, cv=cv_iterator,scoring='mean_squared_error')\n",
    "recursive_selector.fit(second_order.fit_transform(X_train),y_train)\n",
    "print ('Initial number of features : %i' % second_order.fit_transform(X_train).shape[1])\n",
    "print ('Optimal number of features : %i' % recursive_selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recursive_selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for Selecting Stable Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Bootstrap(n, n_iter=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Random sampling with replacement cross-validation generator.\n",
    "    For each iter a sample bootstrap of the indexes [0, n) is\n",
    "    generated and the function returns the obtained sample\n",
    "    and a list of all the excluded indexes.\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "    for j in range(n_iter):\n",
    "        bs = [random.randint(0, n-1) for i in range(n)]\n",
    "        out_bs = list({i for i in range(n)} - set(bs))\n",
    "        yield bs, out_bs\n",
    "            \n",
    "            \n",
    "boot = Bootstrap(n=58, n_iter=5, random_state=101)\n",
    "for train_idx, validation_idx in boot:\n",
    "    print (train_idx, validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot = Bootstrap(n=len(X), n_iter=20, random_state=101)\n",
    "len(X.columns)\n",
    "lm = LinearRegression()\n",
    "bootstrapped_coef = np.zeros((20, len(X.columns)))\n",
    "for k, (train_idx, validation_idx) in enumerate(boot):\n",
    "    lm.fit(X.ix[train_idx,:],y[train_idx])\n",
    "    bootstrapped_coef[k,:] = lm.coef_\n",
    "    \n",
    "print(bootstrapped_coef[:,10])\n",
    "print X.columns.tolist()\n",
    "pd.DataFrame(bootstrapped_coef, columns = X.columns).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_df = pd.DataFrame(bootstrapped_coef, columns = X.columns)\n",
    "sns.distplot(boot_df.std())\n",
    "plt.show()\n",
    "stable_feats = boot_df.std()[boot_df.std() < 1 ].index\n",
    "sns.distplot(boot_df[list(stable_feats)].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_df[list(stable_feats)].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge.fit(second_order.fit_transform(X), y)\n",
    "lm.fit(second_order.fit_transform(X), y)\n",
    "\n",
    "print ('Average coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.mean(lm.coef_), np.mean(ridge.coef_)))\n",
    "print ('Min coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.min(lm.coef_), np.min(ridge.coef_)))\n",
    "print ('Max coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.max(lm.coef_), np.max(ridge.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(corr_dataframe.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_food_des_corr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print correlation(df['n_food_des'], df['num_tracts'])\n",
    "linear_regression = smf.ols(formula='n_food_des ~ num_tracts', data=df)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear_regression = smf.ols(formula='percent_food_desert ~ unemployment_rate', data=df)\n",
    "linear_regression = smf.ols(formula='n_food_des ~ pop2010_in_des+num_tracts+n_urban+n_rural+urban_des+rural_des+Rural+Urban+LILATracts_1And10+high_food_des_prev+cnty_obesity_pct+cnty_obesity_pct_adj+cnty_dm_pct+cnty_dm_pct_adj+cnty_inactive_pct+cnty_inactive_pct_adj+POP2010+OHU2010+NUMGQTRS+HUNVFlag+Adolescent_births+ABR+p_hs_edatt+PC_PHYS_R+DENTIST_R+PSYCH_R+PCT_HSPNC+PCT_WHITE+PCT_BLACK+PCT_ASIAN+PCT_AMIND_ESK+PCT_ISLANDER+PCT_MULTI+PCT_OTHER+PCT_65OVER+PCT_18_64+PCT_UNDR18+PCT_UNDER5+des_percent+unemployment_rate+n_hospitals+mort_30_ami+mort_30_cabg+mort_30_copd+mort_30_hf+mort_30_pn+mort_30_stk+readm_30_ami+readm_30_cabg+readm_30_copd+readm_30_hf+readm_30_hip_knee+readm_30_hosp_wide+readm_30_pn+readm_30_stk+Chlamydia+Tuberculosis+Gonorrhea+HIV+Measles+Mumps+Pertussis+Rubella+opiods_rx_1000+opiods_greater_than_stateavg+MILK_PRICE10+SODA_PRICE10+MILK_SODA_PRICE10+PCH_FFR_07_12+FFR07+FFR12', data=df)\n",
    "#linear_regression = smf.ols(formula='pop2010_in_des ~ n_food_des+num_tracts+n_urban+n_rural+urban_des+rural_des+Rural+Urban+LILATracts_1And10+high_food_des_prev+cnty_obesity_pct+cnty_obesity_pct_adj+cnty_dm_pct+cnty_dm_pct_adj+cnty_inactive_pct+cnty_inactive_pct_adj+POP2010+OHU2010+NUMGQTRS+HUNVFlag+Adolescent_births+ABR+p_hs_edatt+PC_PHYS_R+DENTIST_R+PSYCH_R+PCT_HSPNC+PCT_WHITE+PCT_BLACK+PCT_ASIAN+PCT_AMIND_ESK+PCT_ISLANDER+PCT_MULTI+PCT_OTHER+PCT_65OVER+PCT_18_64+PCT_UNDR18+PCT_UNDER5+des_percent+unemployment_rate+n_hospitals+mort_30_ami+mort_30_cabg+mort_30_copd+mort_30_hf+mort_30_pn+mort_30_stk+readm_30_ami+readm_30_cabg+readm_30_copd+readm_30_hf+readm_30_hip_knee+readm_30_hosp_wide+readm_30_pn+readm_30_stk+Chlamydia+Tuberculosis+Gonorrhea+HIV+Measles+Mumps+Pertussis+Rubella+opiods_rx_1000+opiods_greater_than_stateavg+MILK_PRICE10+SODA_PRICE10+MILK_SODA_PRICE10+PCH_FFR_07_12+FFR07+FFR12', data=df)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "cols2 = [column for column in cols if column != 'n_food_des' and column != 'County']\n",
    "cols2 = [column for column in cols if column not in exclude]\n",
    "names = cols2\n",
    "X = df[cols2].values\n",
    "Y = df[target].values\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "    score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n",
    "                              cv=ShuffleSplit(len(X), 3, .3))\n",
    "    scores.append((round(np.mean(score), 3), names[i]))\n",
    "print sorted(scores, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RandomizedLasso\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Data gets scaled automatically by sklearn's implementation\n",
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "cols2 = [column for column in cols if column != 'n_food_des' and column != 'County']\n",
    "cols2 = [column for column in cols if column not in exclude]\n",
    "names = cols2\n",
    "X = df[cols2].values\n",
    "Y = df[target].values\n",
    "\n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "rlasso.fit(X, Y)\n",
    " \n",
    "print \"Features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    " \n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print \"Features sorted by their rank:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from minepy import MINE\n",
    " \n",
    "np.random.seed(0)\n",
    "ranks = {}\n",
    " \n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    " \n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X, Y)\n",
    "ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    " \n",
    "ridge = Ridge(alpha=7)\n",
    "ridge.fit(X, Y)\n",
    "ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    " \n",
    "lasso = Lasso(alpha=.05)\n",
    "lasso.fit(X, Y)\n",
    "ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    " \n",
    "rlasso = RandomizedLasso(alpha=0.04)\n",
    "rlasso.fit(X, Y)\n",
    "ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n",
    " \n",
    "#stop the search when 5 features are left (they will get equal scores)\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X,Y)\n",
    "ranks[\"RFE\"] = rank_to_dict(map(float, rfe.ranking_), names, order=-1)\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X,Y)\n",
    "ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    " \n",
    "f, pval  = f_regression(X, Y, center=True)\n",
    "ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    " \n",
    "mine = MINE()\n",
    "mic_scores = []\n",
    "for i in range(X.shape[1]):\n",
    "   mine.compute_score(X[:,i], Y)\n",
    "   m = mine.mic()\n",
    "   mic_scores.append(m)\n",
    "\n",
    "ranks[\"MIC\"] = rank_to_dict(mic_scores, names) \n",
    " \n",
    "r = {}\n",
    "for name in names:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print \"\\t%s\" % \"\\t\".join(methods)\n",
    "for name in names:\n",
    "    print \"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                         [ranks[method][name] for method in methods])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(ranks)\n",
    "feature_df\n",
    "# feature_df[:10].plot(rot=90)\n",
    "# feature_df[10:20].plot(rot=90)\n",
    "# feature_df[20:30].plot(rot=90)\n",
    "# feature_df[30:40].plot(rot=90)\n",
    "# feature_df[40:50].plot(rot=90)\n",
    "# feature_df[60:70].plot(rot=90)\n",
    "# feature_df[70:].plot(rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df['Stability'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Alexandre Gramfort and Gael Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n",
    "                                  LassoLarsCV)\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.utils.extmath import pinvh\n",
    "from sklearn.utils import ConvergenceWarning\n",
    "\n",
    "\n",
    "def mutual_incoherence(X_relevant, X_irelevant):\n",
    "    \"\"\"Mutual incoherence, as defined by formula (26a) of [Wainwright2006].\n",
    "    \"\"\"\n",
    "    projector = np.dot(np.dot(X_irelevant.T, X_relevant),\n",
    "                       pinvh(np.dot(X_relevant.T, X_relevant)))\n",
    "    return np.max(np.abs(projector).sum(axis=1))\n",
    "\n",
    "\n",
    "for conditioning in (1, 1e-4):\n",
    "    ###########################################################################\n",
    "#     # Simulate regression data with a correlated design\n",
    "    n_features = 65\n",
    "#     n_relevant_features = 3\n",
    "#     noise_level = .2\n",
    "#     coef_min = .2\n",
    "#     # The Donoho-Tanner phase transition is around n_samples=25: below we\n",
    "#     # will completely fail to recover in the well-conditioned case\n",
    "#     n_samples = 25\n",
    "#     block_size = n_relevant_features\n",
    "\n",
    "#     rng = np.random.RandomState(42)\n",
    "\n",
    "#     # The coefficients of our model\n",
    "    coef = np.zeros(n_features)\n",
    "#     coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)\n",
    "\n",
    "#     # The correlation of our design: variables correlated by blocs of 3\n",
    "#     corr = np.zeros((n_features, n_features))\n",
    "#     for i in range(0, n_features, block_size):\n",
    "#         corr[i:i + block_size, i:i + block_size] = 1 - conditioning\n",
    "#     corr.flat[::n_features + 1] = 1\n",
    "#     corr = linalg.cholesky(corr)\n",
    "\n",
    "#     # Our design\n",
    "#     X = rng.normal(size=(n_samples, n_features))\n",
    "#     X = np.dot(X, corr)\n",
    "#     # Keep [Wainwright2006] (26c) constant\n",
    "#     X[:n_relevant_features] /= np.abs(\n",
    "#         linalg.svdvals(X[:n_relevant_features])).max()\n",
    "#     X = StandardScaler().fit_transform(X.copy())\n",
    "\n",
    "#     # The output variable\n",
    "#     y = np.dot(X, coef)\n",
    "#     y /= np.std(y)\n",
    "#     # We scale the added noise as a function of the average correlation\n",
    "#     # between the design and the output variable\n",
    "#     y += noise_level * rng.normal(size=n_samples)\n",
    "#     mi = mutual_incoherence(X[:, :n_relevant_features],\n",
    "#                             X[:, n_relevant_features:])\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot stability selection path, using a high eps for early stopping\n",
    "    # of the path, to save computation time\n",
    "    alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42,\n",
    "                                                   eps=0.05)\n",
    "\n",
    "    plt.figure()\n",
    "    # We plot the path as a function of alpha/alpha_max to the power 1/3: the\n",
    "    # power 1/3 scales the path less brutally than the log, and enables to\n",
    "    # see the progression along the path\n",
    "    hg = plt.plot(alpha_grid[1:] ** .333, scores_path[coef != 0].T[1:], 'r')\n",
    "    hb = plt.plot(alpha_grid[1:] ** .333, scores_path[coef == 0].T[1:], 'k')\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.xlabel(r'$(\\alpha / \\alpha_{max})^{1/3}$')\n",
    "    plt.ylabel('Stability score: proportion of times selected')\n",
    "    plt.title('Stability Scores Path - Mutual incoherence: %.1f' % mi)\n",
    "    plt.axis('tight')\n",
    "    plt.legend((hg[0], hb[0]), ('relevant features', 'irrelevant features'),\n",
    "               loc='best')\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot the estimated stability scores for a given alpha\n",
    "\n",
    "    # Use 6-fold cross-validation rather than the default 3-fold: it leads to\n",
    "    # a better choice of alpha:\n",
    "    # Stop the user warnings outputs- they are not necessary for the example\n",
    "    # as it is specifically set up to be challenging.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', UserWarning)\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "        lars_cv = LassoLarsCV(cv=6).fit(X, y)\n",
    "\n",
    "    # Run the RandomizedLasso: we use a paths going down to .1*alpha_max\n",
    "    # to avoid exploring the regime in which very noisy variables enter\n",
    "    # the model\n",
    "    alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6)\n",
    "    clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y)\n",
    "    trees = ExtraTreesRegressor(100).fit(X, y)\n",
    "    # Compare with F-score\n",
    "    F, _ = f_regression(X, y)\n",
    "\n",
    "    plt.figure()\n",
    "    for name, score in [('F-test', F),\n",
    "                        ('Stability selection', clf.scores_),\n",
    "                        ('Lasso coefs', np.abs(lars_cv.coef_)),\n",
    "                        ('Trees', trees.feature_importances_),\n",
    "                        ]:\n",
    "        precision, recall, thresholds = precision_recall_curve(coef != 0,\n",
    "                                                               score)\n",
    "        plt.semilogy(np.maximum(score / np.max(score), 1e-4),\n",
    "                     label=\"%s. AUC: %.3f\" % (name, auc(recall, precision)))\n",
    "\n",
    "    plt.plot(np.where(coef != 0)[0], [2e-4] * n_relevant_features, 'mo',\n",
    "             label=\"Ground truth\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    # Plot only the 100 first coefficients\n",
    "    plt.xlim(0, 100)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Feature selection scores - Mutual incoherence: %.1f'\n",
    "              % mi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()\n",
    "#sns.pairplot(df[['n_food_des', 'num_tracts','n_urban','n_rural', 'Rural', 'Urban','cnty_obesity_pct_adj', 'cnty_dm_pct_adj','cnty_inactive_pct_adj', 'POP2010','OHU2010','NUMGQTRS','HUNVFlag','Adolescent_births','ABR']])\n",
    "#import statsmodels.api as sm\n",
    "from scipy.stats.mstats import zscore\n",
    "y = df['n_food_des']\n",
    "x = df['Adolescent_births']\n",
    "sm.OLS(zscore(y), zscore(x)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "def covariance(variable_1, variable_2, bias=0):\n",
    "    observations = float(len(variable_1))\n",
    "    return np.sum((variable_1 - np.mean(variable_1)) * (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n",
    "\n",
    "def standardize(variable):\n",
    "    return (variable - np.mean(variable)) / np.std(variable)\n",
    "\n",
    "def correlation(var1,var2,bias=0):\n",
    "    return covariance(standardize(var1), standardize(var2),bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=order(df, ['County','n_food_des','pop2010_in_des'])\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variables:\n",
    "* 'n_food_des' - What counties tend to have a high number of food deserts?\n",
    "* 'pop2010_in_des' - What counties are most effected by food deserts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[df.columns.tolist()[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [linreg]",
   "language": "python",
   "name": "Python [linreg]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
