{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the dataset we made in our previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/desert/desert_workspace/desert_data/clean_data.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Influenza Death (<65 years of age)':'senior_flu_deaths'}, inplace=True)\n",
    "df.rename(columns={'Varicella Hospitalizations':'varicella_hospitalizations'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols=df.columns.tolist()\n",
    "cols = [col for col in cols if col != 'County']\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['pop2010_in_des','des_percent']].describe()\n",
    "df['at_risk'] = (df['pop2010_in_des'] >= 21217) & (df['des_percent']>=0.079662)\n",
    "df['at_risk'] =df['at_risk'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def draw_histograms(frame, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(16, 12))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        kde = stats.gaussian_kde(frame[var_name])\n",
    "        xx = np.linspace(np.min(frame[var_name]), np.max(frame[var_name]), 1000)\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        frame[var_name].hist(bins=10,ax=ax, normed=True, alpha=0.3)\n",
    "        ax.plot(xx, kde(xx))\n",
    "        ax.set_title(var_name+\" Distribution\")\n",
    "    fig.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "draw_histograms(df, cols[:20], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[20:40], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[40:60], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_histograms(df, cols[60:], 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Normality\n",
    "normaltest returns a 2-tuple of the chi-squared statistic, and the associated p-value. Given the null hypothesis that x came from a normal distribution, the p-value represents the probability that a chi-squared statistic that large (or larger) would be seen.\n",
    "\n",
    "If the p-val is very small, it means it is unlikely that the data came from a normal distribution. (http://stackoverflow.com/questions/12838993/scipy-normaltest-how-is-it-used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xcols=X.columns.tolist()\n",
    "# xcols[0]\n",
    "\n",
    "def assess_normality_of_feats(frame, var):\n",
    "    return stats.normaltest(frame[var])[-1] >= 0.05\n",
    "\n",
    "def obtain_normal_feats(frame, var_lst):\n",
    "    norm = []\n",
    "    for i in var_lst:\n",
    "        #print i, assess_normality_of_feats(frame, i)\n",
    "        if assess_normality_of_feats(frame, i) == True:\n",
    "            norm += [i]\n",
    "    return norm\n",
    "    \n",
    "#norm_no_mc = ['Chlamydia','PCT_65OVER','PCT_HSPNC','PCT_OTHER','PCT_UNDER5','PCT_UNDR18','PCT_WHITE','cnty_dm_pct_adj','cnty_inactive_pct_adj','opiods_rx_1000','readm_30_cabg']\n",
    "\n",
    "norm_cols = obtain_normal_feats(frame=df, var_lst=cols)\n",
    "norm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "observations = len(df)\n",
    "cols = [col for col in cols if col != 'County']\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "X = df[predictors]\n",
    "y  = df[target].values\n",
    "\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.std_\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha=0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "    \n",
    "def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path\n",
    "\n",
    "alpha = 0.02\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"These are our final standardized coefficients: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstandardizing coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "variables = X.columns.tolist()\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing feature importance using standardiazed coefficients\n",
    "\n",
    "#### First let's look at the coefficients obtained without standardization below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "all_vars = predictors.append(target)\n",
    "\n",
    "predictors = X.columns.tolist()\n",
    "\n",
    "dataset = df[predictors]#[all_vars]\n",
    "linear_regression = LinearRegression(normalize=False,fit_intercept=True)\n",
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization,linear_regression)\n",
    "linear_regression.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,linear_regression.coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the coefficients following standardization:\n",
    "\n",
    "\"Having all the predictors on a similar scale now, we can easily provide a more realistic interpretation of each coefcient. Clearly, it appears that a unit change has more impact when it involves the variables POP2010, Adolescent_births, PCT_UNDER5, OHU2010, readm_30_stk, num_urban, FFR12, PCT_UNDR18, n_hospitals, PCT_65OVER, PCT_18_64, readm_30_copd, and NUMGQTRS. The order of the features below show their relevancy when standardized for predicting the number of food deserts present within a county\" (Linear Regression, p.83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs,Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse=True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "alphas, _, coefs = lars_path(X.values, y, method='lasso', verbose=True)\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dataset)#.values)\n",
    "names = dataset.columns.tolist()\n",
    "  \n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \"\\n + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "  \n",
    "print \"Lasso model: \", pretty_print_linear(lasso.coef_, names, sort = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#good predictors\n",
    "sns.lmplot(x='n_rural', y=target, data=df)\n",
    "sns.lmplot(x='HUNVFlag', y=target, data=df)\n",
    "sns.lmplot(x='HIV', y=target, data=df)\n",
    "sns.lmplot(x='PCT_BLACK', y=target, data=df)\n",
    "sns.lmplot(x='PCT_UNDER5', y=target, data=df)\n",
    "sns.lmplot(x='HIV', y=target, data=df)\n",
    "\n",
    "# bad predictor\n",
    "sns.lmplot(x='FFR12', y=target, data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([xx, coefs.T])\n",
    "coefs\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "linear_regression = LinearRegression(normalize=False,fit_intercept=True)\n",
    "\n",
    "def r2_est(X,y):\n",
    "    return r2_score(y,linear_regression.fit(X,y).predict(X))\n",
    "\n",
    "print ('Baseline R2: %0.3f' %  r2_est(X,y))\n",
    "\n",
    "\n",
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X,y) - \\\n",
    "    r2_est(X.values [:,selection],y)) ,dataset.columns[j]))\n",
    "for imp, varname in sorted(r2_impact, reverse=True):\n",
    "    print ('%6.3f %s' %  (imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our dataset by calculating some summary statistics, and making yet another correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_dataframe = df.corr()\n",
    "\n",
    "# compute hierarchical cluster on both rows and columns for correlation matrix and plot heatmap \n",
    "def corr_heatmap(corr_dataframe):\n",
    "    import scipy.cluster.hierarchy as sch\n",
    "    \n",
    "    corr_matrix = np.array(corr_dataframe)\n",
    "    col_names = corr_dataframe.columns\n",
    "    \n",
    "    Y = sch.linkage(corr_matrix, 'single', 'correlation')\n",
    "    Z = sch.dendrogram(Y, color_threshold=0, no_plot=True)['leaves']\n",
    "    corr_matrix = corr_matrix[Z, :]\n",
    "    corr_matrix = corr_matrix[:, Z]\n",
    "    col_names = col_names[Z]\n",
    "    im = plt.imshow(corr_matrix, interpolation='nearest', aspect='auto', cmap='bwr')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(corr_matrix.shape[0]), col_names, rotation='vertical', fontsize=4)\n",
    "    plt.yticks(range(corr_matrix.shape[0]), col_names[::-1], fontsize=4)\n",
    "    \n",
    "# plot\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_high_corr(corr_dataframe, thresh = 0.9):\n",
    "    '''remove predictors with high pairwise correlation'''\n",
    "    abs_corr = np.abs(corr_dataframe).as_matrix() # absolute correlation matrix\n",
    "    col_names = list(corr_dataframe.columns)\n",
    "    \n",
    "    # set up diagonal to 0\n",
    "    np.fill_diagonal(abs_corr, 0)\n",
    "    \n",
    "    print \"Removed predictors (in order): \\n\"\n",
    "    while np.max(abs_corr) >= thresh:\n",
    "        i, j = np.unravel_index(abs_corr.argmax(), abs_corr.shape) # find maximum element\n",
    "        # print abs_corr[i, j]\n",
    "        rdx = which_to_remove(i, j, abs_corr)\n",
    "        # remove corresponding predictor\n",
    "        print col_names.pop(rdx)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 0)\n",
    "        abs_corr = np.delete(abs_corr, rdx, 1)\n",
    "        \n",
    "    return col_names\n",
    "\n",
    "def which_to_remove(i, j, abs_corr):\n",
    "    '''compare two predictors and remove the one with higher abs correlation with other predictors'''\n",
    "    i_absmean = np.mean(abs_corr[i, np.where(abs_corr[i,:] == 0)])\n",
    "    j_absmean = np.mean(abs_corr[j, np.where(abs_corr[j,:] == 0)])\n",
    "    \n",
    "    return i if i_absmean > j_absmean else j\n",
    "\n",
    "# remained predictors\n",
    "col_remained = remove_high_corr(corr_dataframe)\n",
    "data=df[col_remained]\n",
    "corr_dataframe = data.corr()\n",
    "corr_heatmap(corr_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix below shows the dataset WITHOUT the highly correlated features identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = corr_dataframe\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas this correlation matrix shows the dataset with all original features (WITH the highly correlated features identified above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection:\n",
    "#### Pearson Correlation Coefficient\n",
    "* The Pearson Correlation measures the linear relationship between X (the predictor variable) and Y (the response variable). Values can range from -1 (which represents a perfect negative correlation) to 1 (which represents a perfect positive correlation). A negative correlation suggests that as the predictor variable, X, increases in value, we are likely to see a decrease in the value of our target variable, Y. A positive correlation suggests that as the predictor variable increases in value, we are likely to see an increase in the target variable as well. A pearson correlation of 0 suggests there is no relationship between the two variables, and they are of no use in predicting the value of one another. \n",
    "\n",
    "______\n",
    "Now that we have an understanding of what the Pearson Correlation Coefficient represents, let's use the functions defined below to determine what features are correlated with 'n_food_des,' or the number of food deserts in California counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "def covariance(variable_1, variable_2, bias=0):\n",
    "    observations = float(len(variable_1))\n",
    "    return np.sum((variable_1 - np.mean(variable_1)) * (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n",
    "\n",
    "def standardize(variable):\n",
    "    return (variable - np.mean(variable)) / np.std(variable)\n",
    "\n",
    "def correlation(var1,var2,bias=0):\n",
    "    return covariance(standardize(var1), standardize(var2),bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "predictors = [column for column in cols if column not in exclude]\n",
    "names = predictors\n",
    "X = df[predictors].values\n",
    "Y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assess_feats_correlation(predictor_vars, target_var, dataset):\n",
    "    feats_corr = {}\n",
    "    for feature in predictor_vars:\n",
    "        #print feature\n",
    "        #print ('Feature Correlation estimation: %0.5f' % (correlation(dataset[feature], dataset[target_var])))\n",
    "        #print '----'*5\n",
    "        feats_corr[feature] = (target, correlation(dataset[feature], dataset[target_var]))\n",
    "    feats_corr = pd.DataFrame(feats_corr).T\n",
    "    feats_corr.columns = ['target_var', 'correlation']\n",
    "    feats_corr['correlation'] = feats_corr['correlation'].astype(float)\n",
    "    return feats_corr\n",
    "        \n",
    "\n",
    "# target = 'pop2010_in_des'\n",
    "# pop_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "# pop_des_corr['correlation'].plot(kind='bar')\n",
    "# plt.show()\n",
    "\n",
    "target = 'n_food_des' \n",
    "n_food_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "\n",
    "n_food_des_corr['correlation'].plot(kind='bar')\n",
    "n_food_des_corr.sort_values('correlation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_food_des_corr\n",
    "#sns.distplot(df['HIV'])\n",
    "#sns.distplot(standardize(df['HIV']))\n",
    "pearsonr(df['HIV'], df['n_food_des'])\n",
    "\n",
    "def assess_feats_correlation(predictor_vars, target_var, dataset):\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    feats_corr = {}\n",
    "    for feature in predictor_vars:\n",
    "        #print feature\n",
    "        #print ('Feature Correlation estimation: %0.5f' % (correlation(dataset[feature], dataset[target_var])))\n",
    "        #print '----'*5\n",
    "        feats_corr[feature] = (pearsonr(dataset[feature], dataset[target_var]))\n",
    "    feats_corr = pd.DataFrame(feats_corr).T\n",
    "    feats_corr.columns = ['correlation', 'pval']\n",
    "    feats_corr['correlation'] = feats_corr['correlation'].astype(float)\n",
    "    feats_corr['pval'] = feats_corr['pval'].astype(float)\n",
    "    return feats_corr\n",
    "\n",
    "target = 'n_food_des' \n",
    "n_food_des_corr = assess_feats_correlation(predictors, target, df)\n",
    "n_food_des_corr[n_food_des_corr['pval'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keepers = list(n_food_des_corr[n_food_des_corr['pval'] < 0.05].index)\n",
    "len(df[keepers].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for Collinearity\n",
    "\n",
    "#### Definitions:\n",
    "   * Collinearity: Shared variance between two variables\n",
    "   * Multi-collinearity: Shared variance among three or more variables\n",
    "\n",
    "In the previous cells, we assessed each predictor variable's correlation with our target variable using Pearsons Correlation Coefficient. It is important to note however, this approach is only valid when measuring a relationship between two independent variables. \n",
    "\n",
    "Therefore, if we want to include multiple features in our linear regression model, we must assess our features for collinearity. Meaning we must determine whether \"the relation between the variance of the predictor and that of the target is due to unique or shared variance\" (Linear Regression, p72).\n",
    "\n",
    "This can be done by evaluating the partial correlation of each feature we plan to utilize in our model. This value \"represents the exclusive contribution of a variable in predicting the response,\" and will help us avoid misinterpreting collinear features as significant predictors (Linear Regression, p73).\n",
    "\n",
    "Let's asses our features using a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor variables that were shown to have significant correlation with our target \n",
    "# variable independently.\n",
    "X = df[keepers] \n",
    "correlation_matrix = X.corr()\n",
    "print correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    import matplotlib as mpl\n",
    "    variables = data.columns.tolist()\n",
    "    R = np.corrcoef(data, rowvar=0)\n",
    "    R[np.where(np.abs(R)<hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor=False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor=False)\n",
    "    plt.tick_params(axis='both', which='both', bottom='off', \\\n",
    "    top='off', left = 'off', right = 'off')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "visualize_correlation_matrix(X, hurdle=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a cut at 0.5 correlation (which translates into a 25% shared variance), the heat map immediately reveals how senior_flu_deaths and PSYCH_R are not so related to other predictors.\n",
    "\n",
    "\"An even more automatic way to detect such associations (and  gure out numerical problems in a matrix inversion) is to use eigenvectors. Explained in layman's terms, eigenvectors are a very smart way to recombine the variance among the variables, creating new features accumulating all the shared variance. Such recombination\n",
    "can be achieved using the NumPy linalg.eig function, resulting in a vector of eigenvalues (representing the amount of recombined variance for each new variable) and eigenvectors (a matrix telling us how the new variables relate to the old ones)\" (Regression Analysis, p.74):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X, rowvar=0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After extracting the eigenvalues, we print them in descending order and look for any element whose value is near to zero or small compared to the others. Near zero values can represent a real problem for normal equations and other optimization methods based on matrix inversion. Small values represent a high but not critical source of multicollinearity. If you spot any of these low values, keep a note of their index in the list.\"(Regression Analysis, p.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def id_near_zero_eigenvalues(eig_vals):\n",
    "    mc_sources = []\n",
    "    for i in range(len(eigenvalues)):\n",
    "        if eigenvalues[i] <= .1:\n",
    "            mc_sources += [i]\n",
    "    return mc_sources\n",
    "\n",
    "print eigenvalues\n",
    "possible_multicollinear_evals = id_near_zero_eigenvalues(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using their index position in the list of eigenvalues, we can recall their specific vector from eigenvectors, which contains all the variable loadings—that is, the\n",
    "level of association with the original variables. Our eigenvalues dictate that we should investigate the eigenvectors from index 19 to index 34. The functions below use the previously defined id_near_zero_eigenvalues and the eigenvectors to count the number of times a feature was found to have a high amount of collinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def exclude_collinear_vars(eig_vecs, explanatory_vars):\n",
    "    good_vars = []\n",
    "    eig_vecs = list(eig_vecs)\n",
    "    not_collinear = [i for i in range(len(eig_vecs)) if eig_vecs[i] <= 0.1 and eig_vecs[i] >= -0.1]\n",
    "    for i in not_collinear:\n",
    "        good_vars += [explanatory_vars[i]]\n",
    "    return good_vars#var for var in variables if not_collinear\n",
    "\n",
    "def id_best_feats(eig_vals):\n",
    "    import itertools, collections\n",
    "    good_vars = []\n",
    "    assess_further = id_near_zero_eigenvalues(eigenvalues)\n",
    "    for i in assess_further:\n",
    "        good_vars += [exclude_collinear_vars(eigenvectors[:,i], variables)]\n",
    "    counter = collections.Counter(itertools.chain(*good_vars))\n",
    "    times_nomc_found = pd.DataFrame([counter.values()], columns=counter.keys()).T\n",
    "    times_nomc_found.columns = ['count']\n",
    "    return times_nomc_found#good_vars\n",
    "\n",
    "num_no_mc_found = id_best_feats(eigenvalues)\n",
    "\n",
    "num_no_mc_found\n",
    "# import itertools, collections\n",
    "# counter = collections.Counter(itertools.chain(*num_no_mc_found))\n",
    "# times_nomc_found = pd.DataFrame([counter.values()], columns=counter.keys()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_nomc_found.columns = ['count']\n",
    "times_nomc_found.describe()\n",
    "least_redundant_feats = times_nomc_found[times_nomc_found >= 10].dropna().index\n",
    "df[least_redundant_feats]\n",
    "feats_inc_y = [val for val in least_redundant_feats]\n",
    "feats_inc_y += ['n_food_des']\n",
    "feats_inc_y\n",
    "corrmat = df[feats_inc_y].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap using seaborn, and add a title to the plot\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "ax.set_title('CA Food Desert Data Correlations')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "second_order=PolynomialFeatures(degree=2, interaction_only=False)\n",
    "third_order=PolynomialFeatures(degree=3, interaction_only=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, Y, test_size=0.30, random_state=1)\n",
    "lm = LinearRegression()\n",
    "cv_iterator = KFold(n=len(X_train), n_folds=10, shuffle=True, random_state=101)\n",
    "recursive_selector = RFECV(estimator=lm, step=1, cv=cv_iterator,scoring='mean_squared_error')\n",
    "recursive_selector.fit(second_order.fit_transform(X_train),y_train)\n",
    "print ('Initial number of features : %i' % second_order.fit_transform(X_train).shape[1])\n",
    "print ('Optimal number of features : %i' % recursive_selector.n_features_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recursive_selector.ranking_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for Selecting Stable Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def Bootstrap(n, n_iter=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Random sampling with replacement cross-validation generator.\n",
    "    For each iter a sample bootstrap of the indexes [0, n) is\n",
    "    generated and the function returns the obtained sample\n",
    "    and a list of all the excluded indexes.\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "    for j in range(n_iter):\n",
    "        bs = [random.randint(0, n-1) for i in range(n)]\n",
    "        out_bs = list({i for i in range(n)} - set(bs))\n",
    "        yield bs, out_bs\n",
    "            \n",
    "            \n",
    "boot = Bootstrap(n=58, n_iter=5, random_state=101)\n",
    "for train_idx, validation_idx in boot:\n",
    "    print (train_idx, validation_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot = Bootstrap(n=len(X), n_iter=20, random_state=101)\n",
    "len(X.columns)\n",
    "lm = LinearRegression()\n",
    "bootstrapped_coef = np.zeros((20, len(X.columns)))\n",
    "for k, (train_idx, validation_idx) in enumerate(boot):\n",
    "    lm.fit(X.ix[train_idx,:],y[train_idx])\n",
    "    bootstrapped_coef[k,:] = lm.coef_\n",
    "    \n",
    "print(bootstrapped_coef[:,10])\n",
    "print X.columns.tolist()\n",
    "pd.DataFrame(bootstrapped_coef, columns = X.columns).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_df = pd.DataFrame(bootstrapped_coef, columns = X.columns)\n",
    "sns.distplot(boot_df.std())\n",
    "plt.show()\n",
    "stable_feats = boot_df.std()[boot_df.std() < 1 ].index\n",
    "sns.distplot(boot_df[list(stable_feats)].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_df[list(stable_feats)].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge.fit(second_order.fit_transform(X), y)\n",
    "lm.fit(second_order.fit_transform(X), y)\n",
    "\n",
    "print ('Average coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.mean(lm.coef_), np.mean(ridge.coef_)))\n",
    "print ('Min coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.min(lm.coef_), np.min(ridge.coef_)))\n",
    "print ('Max coefficient: Non regularized = %0.3f Ridge = %0.3f' % (np.max(lm.coef_), np.max(ridge.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(corr_dataframe.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_food_des_corr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print correlation(df['n_food_des'], df['num_tracts'])\n",
    "linear_regression = smf.ols(formula='n_food_des ~ num_tracts', data=df)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear_regression = smf.ols(formula='percent_food_desert ~ unemployment_rate', data=df)\n",
    "linear_regression = smf.ols(formula='n_food_des ~ pop2010_in_des+num_tracts+n_urban+n_rural+urban_des+rural_des+Rural+Urban+LILATracts_1And10+high_food_des_prev+cnty_obesity_pct+cnty_obesity_pct_adj+cnty_dm_pct+cnty_dm_pct_adj+cnty_inactive_pct+cnty_inactive_pct_adj+POP2010+OHU2010+NUMGQTRS+HUNVFlag+Adolescent_births+ABR+p_hs_edatt+PC_PHYS_R+DENTIST_R+PSYCH_R+PCT_HSPNC+PCT_WHITE+PCT_BLACK+PCT_ASIAN+PCT_AMIND_ESK+PCT_ISLANDER+PCT_MULTI+PCT_OTHER+PCT_65OVER+PCT_18_64+PCT_UNDR18+PCT_UNDER5+des_percent+unemployment_rate+n_hospitals+mort_30_ami+mort_30_cabg+mort_30_copd+mort_30_hf+mort_30_pn+mort_30_stk+readm_30_ami+readm_30_cabg+readm_30_copd+readm_30_hf+readm_30_hip_knee+readm_30_hosp_wide+readm_30_pn+readm_30_stk+Chlamydia+Tuberculosis+Gonorrhea+HIV+Measles+Mumps+Pertussis+Rubella+opiods_rx_1000+opiods_greater_than_stateavg+MILK_PRICE10+SODA_PRICE10+MILK_SODA_PRICE10+PCH_FFR_07_12+FFR07+FFR12', data=df)\n",
    "#linear_regression = smf.ols(formula='pop2010_in_des ~ n_food_des+num_tracts+n_urban+n_rural+urban_des+rural_des+Rural+Urban+LILATracts_1And10+high_food_des_prev+cnty_obesity_pct+cnty_obesity_pct_adj+cnty_dm_pct+cnty_dm_pct_adj+cnty_inactive_pct+cnty_inactive_pct_adj+POP2010+OHU2010+NUMGQTRS+HUNVFlag+Adolescent_births+ABR+p_hs_edatt+PC_PHYS_R+DENTIST_R+PSYCH_R+PCT_HSPNC+PCT_WHITE+PCT_BLACK+PCT_ASIAN+PCT_AMIND_ESK+PCT_ISLANDER+PCT_MULTI+PCT_OTHER+PCT_65OVER+PCT_18_64+PCT_UNDR18+PCT_UNDER5+des_percent+unemployment_rate+n_hospitals+mort_30_ami+mort_30_cabg+mort_30_copd+mort_30_hf+mort_30_pn+mort_30_stk+readm_30_ami+readm_30_cabg+readm_30_copd+readm_30_hf+readm_30_hip_knee+readm_30_hosp_wide+readm_30_pn+readm_30_stk+Chlamydia+Tuberculosis+Gonorrhea+HIV+Measles+Mumps+Pertussis+Rubella+opiods_rx_1000+opiods_greater_than_stateavg+MILK_PRICE10+SODA_PRICE10+MILK_SODA_PRICE10+PCH_FFR_07_12+FFR07+FFR12', data=df)\n",
    "fitted_model = linear_regression.fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "cols2 = [column for column in cols if column != 'n_food_des' and column != 'County']\n",
    "cols2 = [column for column in cols if column not in exclude]\n",
    "names = cols2\n",
    "X = df[cols2].values\n",
    "Y = df[target].values\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "    score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n",
    "                              cv=ShuffleSplit(len(X), 3, .3))\n",
    "    scores.append((round(np.mean(score), 3), names[i]))\n",
    "print sorted(scores, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RandomizedLasso\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Data gets scaled automatically by sklearn's implementation\n",
    "cols = df.columns.tolist()\n",
    "target = 'n_food_des' \n",
    "exclude = ['County','n_food_des','rural_des','urban_des','pop2010_in_des','LILATracts_1And10','des_percent','cnty_obesity_pct','cnty_inactive_pct','cnty_dm_pct', 'high_food_des_prev']\n",
    "cols2 = [column for column in cols if column != 'n_food_des' and column != 'County']\n",
    "cols2 = [column for column in cols if column not in exclude]\n",
    "names = cols2\n",
    "X = df[cols2].values\n",
    "Y = df[target].values\n",
    "\n",
    "rlasso = RandomizedLasso(alpha=0.025)\n",
    "rlasso.fit(X, Y)\n",
    " \n",
    "print \"Features sorted by their score:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    " \n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    " \n",
    "print \"Features sorted by their rank:\"\n",
    "print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from minepy import MINE\n",
    " \n",
    "np.random.seed(0)\n",
    "ranks = {}\n",
    " \n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    " \n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X, Y)\n",
    "ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    " \n",
    "ridge = Ridge(alpha=7)\n",
    "ridge.fit(X, Y)\n",
    "ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    " \n",
    "lasso = Lasso(alpha=.05)\n",
    "lasso.fit(X, Y)\n",
    "ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    " \n",
    "rlasso = RandomizedLasso(alpha=0.04)\n",
    "rlasso.fit(X, Y)\n",
    "ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n",
    " \n",
    "#stop the search when 5 features are left (they will get equal scores)\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X,Y)\n",
    "ranks[\"RFE\"] = rank_to_dict(map(float, rfe.ranking_), names, order=-1)\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X,Y)\n",
    "ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    " \n",
    "f, pval  = f_regression(X, Y, center=True)\n",
    "ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    " \n",
    "mine = MINE()\n",
    "mic_scores = []\n",
    "for i in range(X.shape[1]):\n",
    "   mine.compute_score(X[:,i], Y)\n",
    "   m = mine.mic()\n",
    "   mic_scores.append(m)\n",
    "\n",
    "ranks[\"MIC\"] = rank_to_dict(mic_scores, names) \n",
    " \n",
    "r = {}\n",
    "for name in names:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print \"\\t%s\" % \"\\t\".join(methods)\n",
    "for name in names:\n",
    "    print \"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                         [ranks[method][name] for method in methods])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(ranks)\n",
    "feature_df\n",
    "# feature_df[:10].plot(rot=90)\n",
    "# feature_df[10:20].plot(rot=90)\n",
    "# feature_df[20:30].plot(rot=90)\n",
    "# feature_df[30:40].plot(rot=90)\n",
    "# feature_df[40:50].plot(rot=90)\n",
    "# feature_df[60:70].plot(rot=90)\n",
    "# feature_df[70:].plot(rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df['Stability'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Alexandre Gramfort and Gael Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n",
    "                                  LassoLarsCV)\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.utils.extmath import pinvh\n",
    "from sklearn.utils import ConvergenceWarning\n",
    "\n",
    "\n",
    "def mutual_incoherence(X_relevant, X_irelevant):\n",
    "    \"\"\"Mutual incoherence, as defined by formula (26a) of [Wainwright2006].\n",
    "    \"\"\"\n",
    "    projector = np.dot(np.dot(X_irelevant.T, X_relevant),\n",
    "                       pinvh(np.dot(X_relevant.T, X_relevant)))\n",
    "    return np.max(np.abs(projector).sum(axis=1))\n",
    "\n",
    "\n",
    "for conditioning in (1, 1e-4):\n",
    "    ###########################################################################\n",
    "#     # Simulate regression data with a correlated design\n",
    "    n_features = 65\n",
    "#     n_relevant_features = 3\n",
    "#     noise_level = .2\n",
    "#     coef_min = .2\n",
    "#     # The Donoho-Tanner phase transition is around n_samples=25: below we\n",
    "#     # will completely fail to recover in the well-conditioned case\n",
    "#     n_samples = 25\n",
    "#     block_size = n_relevant_features\n",
    "\n",
    "#     rng = np.random.RandomState(42)\n",
    "\n",
    "#     # The coefficients of our model\n",
    "    coef = np.zeros(n_features)\n",
    "#     coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)\n",
    "\n",
    "#     # The correlation of our design: variables correlated by blocs of 3\n",
    "#     corr = np.zeros((n_features, n_features))\n",
    "#     for i in range(0, n_features, block_size):\n",
    "#         corr[i:i + block_size, i:i + block_size] = 1 - conditioning\n",
    "#     corr.flat[::n_features + 1] = 1\n",
    "#     corr = linalg.cholesky(corr)\n",
    "\n",
    "#     # Our design\n",
    "#     X = rng.normal(size=(n_samples, n_features))\n",
    "#     X = np.dot(X, corr)\n",
    "#     # Keep [Wainwright2006] (26c) constant\n",
    "#     X[:n_relevant_features] /= np.abs(\n",
    "#         linalg.svdvals(X[:n_relevant_features])).max()\n",
    "#     X = StandardScaler().fit_transform(X.copy())\n",
    "\n",
    "#     # The output variable\n",
    "#     y = np.dot(X, coef)\n",
    "#     y /= np.std(y)\n",
    "#     # We scale the added noise as a function of the average correlation\n",
    "#     # between the design and the output variable\n",
    "#     y += noise_level * rng.normal(size=n_samples)\n",
    "#     mi = mutual_incoherence(X[:, :n_relevant_features],\n",
    "#                             X[:, n_relevant_features:])\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot stability selection path, using a high eps for early stopping\n",
    "    # of the path, to save computation time\n",
    "    alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42,\n",
    "                                                   eps=0.05)\n",
    "\n",
    "    plt.figure()\n",
    "    # We plot the path as a function of alpha/alpha_max to the power 1/3: the\n",
    "    # power 1/3 scales the path less brutally than the log, and enables to\n",
    "    # see the progression along the path\n",
    "    hg = plt.plot(alpha_grid[1:] ** .333, scores_path[coef != 0].T[1:], 'r')\n",
    "    hb = plt.plot(alpha_grid[1:] ** .333, scores_path[coef == 0].T[1:], 'k')\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.xlabel(r'$(\\alpha / \\alpha_{max})^{1/3}$')\n",
    "    plt.ylabel('Stability score: proportion of times selected')\n",
    "    plt.title('Stability Scores Path - Mutual incoherence: %.1f' % mi)\n",
    "    plt.axis('tight')\n",
    "    plt.legend((hg[0], hb[0]), ('relevant features', 'irrelevant features'),\n",
    "               loc='best')\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot the estimated stability scores for a given alpha\n",
    "\n",
    "    # Use 6-fold cross-validation rather than the default 3-fold: it leads to\n",
    "    # a better choice of alpha:\n",
    "    # Stop the user warnings outputs- they are not necessary for the example\n",
    "    # as it is specifically set up to be challenging.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', UserWarning)\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "        lars_cv = LassoLarsCV(cv=6).fit(X, y)\n",
    "\n",
    "    # Run the RandomizedLasso: we use a paths going down to .1*alpha_max\n",
    "    # to avoid exploring the regime in which very noisy variables enter\n",
    "    # the model\n",
    "    alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6)\n",
    "    clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y)\n",
    "    trees = ExtraTreesRegressor(100).fit(X, y)\n",
    "    # Compare with F-score\n",
    "    F, _ = f_regression(X, y)\n",
    "\n",
    "    plt.figure()\n",
    "    for name, score in [('F-test', F),\n",
    "                        ('Stability selection', clf.scores_),\n",
    "                        ('Lasso coefs', np.abs(lars_cv.coef_)),\n",
    "                        ('Trees', trees.feature_importances_),\n",
    "                        ]:\n",
    "        precision, recall, thresholds = precision_recall_curve(coef != 0,\n",
    "                                                               score)\n",
    "        plt.semilogy(np.maximum(score / np.max(score), 1e-4),\n",
    "                     label=\"%s. AUC: %.3f\" % (name, auc(recall, precision)))\n",
    "\n",
    "    plt.plot(np.where(coef != 0)[0], [2e-4] * n_relevant_features, 'mo',\n",
    "             label=\"Ground truth\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    # Plot only the 100 first coefficients\n",
    "    plt.xlim(0, 100)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Feature selection scores - Mutual incoherence: %.1f'\n",
    "              % mi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()\n",
    "#sns.pairplot(df[['n_food_des', 'num_tracts','n_urban','n_rural', 'Rural', 'Urban','cnty_obesity_pct_adj', 'cnty_dm_pct_adj','cnty_inactive_pct_adj', 'POP2010','OHU2010','NUMGQTRS','HUNVFlag','Adolescent_births','ABR']])\n",
    "#import statsmodels.api as sm\n",
    "from scipy.stats.mstats import zscore\n",
    "y = df['n_food_des']\n",
    "x = df['Adolescent_births']\n",
    "sm.OLS(zscore(y), zscore(x)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame \n",
    "\n",
    "def covariance(variable_1, variable_2, bias=0):\n",
    "    observations = float(len(variable_1))\n",
    "    return np.sum((variable_1 - np.mean(variable_1)) * (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))\n",
    "\n",
    "def standardize(variable):\n",
    "    return (variable - np.mean(variable)) / np.std(variable)\n",
    "\n",
    "def correlation(var1,var2,bias=0):\n",
    "    return covariance(standardize(var1), standardize(var2),bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=order(df, ['County','n_food_des','pop2010_in_des'])\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variables:\n",
    "* 'n_food_des' - What counties tend to have a high number of food deserts?\n",
    "* 'pop2010_in_des' - What counties are most effected by food deserts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[df.columns.tolist()[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
